#!/bin/zsh

# git-upload
#
# Usage:
#   git-upload [-ai|--aiDiffCommitMsg [ai-context]] [commit message]
#
# Behaviour:
#   - If --aiDiffCommitMsg / -ai is passed, the script will generate a commit
#     message from the current diff using GitHub Copilot CLI by default.
#   - The first non-flag argument after -ai (if any) is treated as optional
#     extra context to append to the default AI prompt.
#   - The next non-flag argument (if any) is used as a fallback/manual
#     commit message if AI is unavailable or fails.
#   - Without -ai, the first non-flag argument is the commit message,
#     defaulting to "default commit message".
#
# AI integration:
#   Default behaviour (no configuration needed as long as `copilot` is
#   installed and on PATH):
#     - The script builds a prompt like:
#         "examine differences between the last known push to this repository
#          and the current state, write a clean concise one-three line message
#          for this diff"
#       and, if you provided [ai-context] after -ai, appends that text.
#     - It then calls GitHub Copilot CLI non-interactively:
#         copilot -s --allow-all-tools --allow-all-paths -p "$GIT_UPLOAD_AI_PROMPT"
#
#   Advanced: you may optionally override the full AI command via
#   GIT_UPLOAD_AI_CMD for a single shell/session, but this is not required.

set -euo pipefail

use_ai=false
user_msg=""
ai_extra_context=""

DEFAULT_AI_CMD='copilot -s --allow-all-tools --allow-all-paths -p "$GIT_UPLOAD_AI_PROMPT"'

expect_ai_context_next=false

for arg in "$@"; do
	case "$arg" in
		--aiDiffCommitMsg|-ai)
			use_ai=true
			expect_ai_context_next=true
			;;
		--*)
			# Ignore other flags for now
			;;
		*)
			if [ "$expect_ai_context_next" = true ]; then
				# This non-flag arg immediately after -ai/--aiDiffCommitMsg
				# is treated as optional extra AI context.
				ai_extra_context="$arg"
				expect_ai_context_next=false
			elif [ -z "$user_msg" ]; then
				user_msg="$arg"
			fi
			;;
	esac
done


detect_vscode_test_task() {
	# Best-effort parser for .vscode/tasks.json (JSON/JSONC) to find a "test" task.
	# Prints two lines on success:
	#  1) command line (shell-quoted where needed)
	#  2) cwd (may be empty)
	local repo_root="$1"
	local tasks_file="$repo_root/.vscode/tasks.json"
	[ -f "$tasks_file" ] || return 1

	# Strip JSONC-style comments (best effort): remove /* ... */ blocks (line-based)
	# and // full-line comments. This stays dependency-free.
	local stripped
	stripped=$(sed -e '/\/\*/,/\*\//d' -e '/^[[:space:]]*\/\//d' "$tasks_file")

	printf '%s\n' "$stripped" | awk -v repo_root="$repo_root" '
		function countch(s, c,    i, n) { n=0; for (i=1; i<=length(s); i++) if (substr(s,i,1)==c) n++; return n }
		function ltrim(s) { sub(/^[[:space:]]+/, "", s); return s }
		function rtrim(s) { sub(/[[:space:]]+$/, "", s); return s }
		function trim(s) { return rtrim(ltrim(s)) }
		function subst_vars(s) {
			gsub(/\$\{workspaceFolder\}/, repo_root, s)
			gsub(/\$\{workspaceRoot\}/, repo_root, s)
			return s
		}
		function extract_string(line, key,    t) {
			t=line
			if (index(t, "\"" key "\"") == 0) return ""
			sub(".*\"" key "\"[[:space:]]*:[[:space:]]*\"", "", t)
			sub("\".*", "", t)
			return t
		}
		BEGIN {
			in_tasks=0; found_tasks=0;
			in_obj=0; depth=0;
			in_args=0;
			label=""; group=""; command=""; cwd=""; args="";
			best_cmd=""; best_cwd=""; best_rank=999;
		}
		{
			line=$0
			if (!in_tasks) {
				if (line ~ /"tasks"[[:space:]]*:/) found_tasks=1
				if (found_tasks && line ~ /\[/) in_tasks=1
			}
			if (!in_tasks) next

			if (!in_obj && line ~ /\{/) {
				in_obj=1; depth=0; in_args=0
				label=""; group=""; command=""; cwd=""; args=""
			}

			if (in_obj) {
				if (label=="") { tmp=extract_string(line, "label"); if (tmp!="") label=tmp }
				if (command=="") { tmp=extract_string(line, "command"); if (tmp!="") command=tmp }
				if (group=="") { tmp=extract_string(line, "group"); if (tmp!="") group=tmp }
				if (group=="" && line ~ /"kind"[[:space:]]*:[[:space:]]*"test"/) group="test"
				if (cwd=="") { tmp=extract_string(line, "cwd"); if (tmp!="") cwd=tmp }

				# args: capture quoted strings inside args array (best-effort)
				if (!in_args && line ~ /"args"[[:space:]]*:[[:space:]]*\[/) {
					in_args=1
					# Drop everything up to the opening '[' to avoid capturing the key name.
					sub(/.*\[/, "", line)
				}
				if (in_args) {
					work=line
					while (index(work, "\"") > 0) {
						i=index(work, "\"")
						work=substr(work, i+1)
						j=index(work, "\"")
						if (j==0) break
						arg=substr(work, 1, j-1)
						arg=subst_vars(arg)
						q=arg
						gsub(/\\/, "\\\\", q)
						gsub(/\"/, "\\\"", q)
						args = args " " "\"" q "\""
						work=substr(work, j+1)
					}
					if (line ~ /\]/) in_args=0
				}

				depth += countch($0, "{") - countch($0, "}")
				if (depth <= 0 && $0 ~ /}/) {
					lbl=tolower(trim(label))
					grp=tolower(trim(group))
					cmd=subst_vars(command)
					cw=subst_vars(cwd)
					rank=999
					if (lbl == "test") rank=0
					else if (grp == "test") rank=1
					if (cmd != "" && rank < best_rank) {
						best_rank=rank
						best_cmd=trim(cmd) args
						best_cwd=trim(cw)
					}
					in_obj=0
				}
			}
		}
		END {
			if (best_cmd != "") {
				print trim(best_cmd)
				print trim(best_cwd)
				exit 0
			}
			exit 1
		}
	'
}

detect_test_cmd() {
	local repo_root="$1"
	if [ -z "${repo_root// /}" ]; then
		repo_root="."
	fi

	repo_has_python_tests() {
		local root="$1"
		find "$root" \
			-type f \
			\( -name 'test_*.py' -o -name '*_test.py' \) \
			-not -path '*/.venv/*' \
			-not -path '*/venv/*' \
			-not -path '*/.tox/*' \
			-not -path '*/.pytest_cache/*' \
			-not -path '*/.mypy_cache/*' \
			-not -path '*/.git/*' \
			-maxdepth 6 \
			-print 2>/dev/null \
			| head -n 1 \
			| grep -q '.'
	}

	repo_has_go_tests() {
		local root="$1"
		find "$root" \
			-type f \
			-name '*_test.go' \
			-not -path '*/.git/*' \
			-maxdepth 6 \
			-print 2>/dev/null \
			| head -n 1 \
			| grep -q '.'
	}

	package_json_test_script() {
		# Best-effort: detect if package.json defines a non-placeholder scripts.test.
		# Returns 0 if present, 1 if missing/placeholder.
		local pkg="$1"
		[ -f "$pkg" ] || return 1

		local script
		script=$(awk '
			function countch(s, c,    i, n) { n=0; for (i=1; i<=length(s); i++) if (substr(s,i,1)==c) n++; return n }
			function ltrim(s) { sub(/^[[:space:]]+/, "", s); return s }
			function rtrim(s) { sub(/[[:space:]]+$/, "", s); return s }
			function trim(s) { return rtrim(ltrim(s)) }
			function extract_test_value(line,    t) {
				t=line
				if (index(t, "\"test\"") == 0) return ""
				sub(".*\"test\"[[:space:]]*:[[:space:]]*\"", "", t)
				sub("\".*", "", t)
				return t
			}
			BEGIN { in_scripts=0; depth=0; found=0; val="" }
			{
				line=$0
				if (!in_scripts) {
					if (line ~ /\"scripts\"[[:space:]]*:/) {
						in_scripts=1
						# allow same-line object start
						depth += countch(line, "{") - countch(line, "}")
						# do not next; scripts/test may be on the same line in minified JSON
					}
					if (!in_scripts) next
				}

				depth += countch(line, "{") - countch(line, "}")
				tmp=extract_test_value(line)
				if (tmp != "") { found=1; val=tmp }
				if (depth <= 0) { in_scripts=0 }
			}
			END {
				if (found) print trim(val)
			}
		' "$pkg" 2>/dev/null || true)

		if [ -z "${script// /}" ]; then
			return 1
		fi
		if printf '%s' "$script" | grep -qi 'no test specified'; then
			return 1
		fi
		return 0
	}

	repo_has_java_tests() {
		# Conservative: require a src/test tree and at least one test-like file.
		local root="$1"
		find "$root" \
			-type f \
			\(
				-path '*/src/test/*' \
				-o -path '*/src/androidTest/*'
			\) \
			\(
				-name '*Test.java' -o -name '*Tests.java' -o -name '*IT.java' \
				-o -name '*Test.kt' -o -name '*Tests.kt' -o -name '*IT.kt' \
				-o -name '*Test.groovy' -o -name '*Tests.groovy' -o -name '*IT.groovy'
			\) \
			-not -path '*/build/*' \
			-not -path '*/target/*' \
			-not -path '*/.git/*' \
			-maxdepth 8 \
			-print 2>/dev/null \
			| head -n 1 \
			| grep -q '.'
	}

	repo_has_rust_tests() {
		# Rust can have inline #[test] modules without a tests/ folder.
		# Use a shallow heuristic to avoid running cargo test in repos with no code.
		local root="$1"
		if [ -d "$root/tests" ]; then
			find "$root/tests" -type f -maxdepth 3 -print 2>/dev/null | head -n 1 | grep -q '.' && return 0
		fi
		find "$root" \
			-type f \
			\( -path '*/src/*.rs' -o -path '*/src/*/*.rs' \) \
			-not -path '*/target/*' \
			-not -path '*/.git/*' \
			-maxdepth 6 \
			-print 2>/dev/null \
			| while read -r f; do
				grep -qE '#\[[[:space:]]*test\]' "$f" 2>/dev/null && { printf '%s\n' "$f"; break; }
			done \
			| head -n 1 \
			| grep -q '.'
	}

	is_dotnet_test_project() {
		local csproj="$1"
		[ -f "$csproj" ] || return 1
		grep -qE 'Microsoft\.NET\.Test\.Sdk|<IsTestProject>[[:space:]]*true[[:space:]]*</IsTestProject>' "$csproj" 2>/dev/null
	}

	# Prints two lines on success:
	#  1) command line
	#  2) cwd (may be empty)
	#
	# Priority:
	#  1) Explicit env var (caller-controlled)
	#  2) Per-repo git config
	#  3) VS Code workspace test task (.vscode/tasks.json group: test)
	#  4) Repo-local test runner / Makefile
	#  5) Simple heuristics for common stacks
	if [ -n "${GIT_UPLOAD_TEST_CMD-}" ]; then
		printf '%s\n' "$GIT_UPLOAD_TEST_CMD"
		printf '%s' ""
		return 0
	fi

	local cfg_cmd
	cfg_cmd=$(git config --get git-upload.testCmd 2>/dev/null || echo "")
	if [ -n "${cfg_cmd// /}" ]; then
		printf '%s\n' "$cfg_cmd"
		printf '%s' ""
		return 0
	fi

	local vs_task
	if vs_task=$(detect_vscode_test_task "$repo_root" 2>/dev/null); then
		# Already prints two lines: cmdline then cwd.
		printf '%s' "$vs_task"
		return 0
	fi

	if [ -x "$repo_root/scripts/test.sh" ]; then
		printf '%s\n' './scripts/test.sh'
		printf '%s' ""
		return 0
	fi

	if { [ -f "$repo_root/Makefile" ] || [ -f "$repo_root/makefile" ]; } && command -v make >/dev/null 2>&1; then
		local mk
		mk=$([ -f "$repo_root/Makefile" ] && echo "$repo_root/Makefile" || echo "$repo_root/makefile")
		if grep -qE '^test:' "$mk" 2>/dev/null; then
			printf '%s\n' 'make test'
			printf '%s' ""
			return 0
		fi
	fi

	if [ -f "$repo_root/package.json" ]; then
		# Only run JS tests if a real scripts.test exists.
		if package_json_test_script "$repo_root/package.json"; then
			if [ -f "$repo_root/pnpm-lock.yaml" ] && command -v pnpm >/dev/null 2>&1; then
				printf '%s\n' 'pnpm test --silent'
				printf '%s' ""
				return 0
			fi
			if [ -f "$repo_root/yarn.lock" ] && command -v yarn >/dev/null 2>&1; then
				printf '%s\n' 'yarn test --silent'
				printf '%s' ""
				return 0
			fi
			if command -v npm >/dev/null 2>&1; then
				printf '%s\n' 'npm test --silent'
				printf '%s' ""
				return 0
			fi
		fi
	fi

	if { [ -f "$repo_root/pyproject.toml" ] || [ -f "$repo_root/pytest.ini" ] || [ -d "$repo_root/tests" ]; } && command -v pytest >/dev/null 2>&1; then
		# Pytest exits non-zero when no tests are collected; require at least one test file.
		if repo_has_python_tests "$repo_root"; then
			printf '%s\n' 'pytest -q'
			printf '%s' ""
			return 0
		fi
	fi

	if [ -f "$repo_root/go.mod" ] && command -v go >/dev/null 2>&1; then
		# Avoid running go test in repos with zero *_test.go files.
		if repo_has_go_tests "$repo_root"; then
			printf '%s\n' 'go test ./...'
			printf '%s' ""
			return 0
		fi
	fi

	if [ -f "$repo_root/Cargo.toml" ] && command -v cargo >/dev/null 2>&1; then
		# cargo test succeeds with zero tests, but only run when tests are likely present.
		if repo_has_rust_tests "$repo_root"; then
			printf '%s\n' 'cargo test -q'
			printf '%s' ""
			return 0
		fi
	fi

	# .NET (C#)
	if command -v dotnet >/dev/null 2>&1; then
		local sln
		sln=$(find "$repo_root" -maxdepth 2 -name '*.sln' -print 2>/dev/null | head -n 1 || true)
		if [ -n "${sln// /}" ]; then
			# Only run dotnet test when a test project exists (otherwise dotnet returns failure).
			local any_test_proj
			any_test_proj=$(find "$repo_root" -maxdepth 3 -name '*.csproj' -print 2>/dev/null | while read -r p; do
				if is_dotnet_test_project "$p"; then
					printf '%s\n' "$p"
					break
				fi
			done)
			if [ -n "${any_test_proj// /}" ]; then
				printf '%s\n' "dotnet test \"$sln\" --nologo"
				printf '%s' ""
				return 0
			fi
		fi

		local csproj
		csproj=$(find "$repo_root" -maxdepth 3 -name '*.csproj' -print 2>/dev/null | head -n 1 || true)
		if [ -n "${csproj// /}" ] && is_dotnet_test_project "$csproj"; then
			printf '%s\n' "dotnet test \"$csproj\" --nologo"
			printf '%s' ""
			return 0
		fi
	fi

	if [ -f "$repo_root/pom.xml" ] && command -v mvn >/dev/null 2>&1; then
		if repo_has_java_tests "$repo_root"; then
			printf '%s\n' 'mvn test -q'
			printf '%s' ""
			return 0
		fi
	fi

	if [ -x "$repo_root/gradlew" ]; then
		if repo_has_java_tests "$repo_root"; then
			printf '%s\n' './gradlew test -q'
			printf '%s' ""
			return 0
		fi
	fi
	if { [ -f "$repo_root/build.gradle" ] || [ -f "$repo_root/build.gradle.kts" ]; } && command -v gradle >/dev/null 2>&1; then
		if repo_has_java_tests "$repo_root"; then
			printf '%s\n' 'gradle test -q'
			printf '%s' ""
			return 0
		fi
	fi

	return 1
}

summarize_test_output() {
	# Args:
	#  1) test_cmd
	#  2) exit_code
	#  3) path to output file
	local test_cmd="$1"
	local exit_code="$2"
	local output_file="$3"

	local test_status
	if [ "$exit_code" -eq 0 ]; then
		test_status='pass'
	else
		test_status='fail'
	fi

	# If the runner emitted a machine-parseable summary, prefer it.
	# Format:
	#   TEST_SUMMARY: pass 7/7
	#   TEST_SUMMARY: fail 2/7
	#   TEST_FAIL: <name>
	local summary_line
	summary_line=$(grep '^TEST_SUMMARY: ' "$output_file" | tail -n 1 || true)
	if [ -n "${summary_line// /}" ]; then
		local sum_status
		local sum_counts
		sum_status=$(printf '%s\n' "$summary_line" | awk '{print $2}')
		sum_counts=$(printf '%s\n' "$summary_line" | awk '{print $3}')
		if [ -n "${sum_status// /}" ] && [ -n "${sum_counts// /}" ]; then
			local header="Testing: ${sum_status} (${sum_counts})"
			if [ "$sum_status" = "fail" ]; then
				local sum_failures
				sum_failures=$(grep '^TEST_FAIL: ' "$output_file" | sed 's/^TEST_FAIL: //' | head -n 10)
				if [ -n "${sum_failures// /}" ]; then
					printf '%s\n' "$header"
					printf '%s\n' "$sum_failures" | sed 's/^/- /'
					return 0
				fi
			fi
			printf '%s' "$header"
			return 0
		fi
	fi

	local passed=""
	local failed=""
	local total=""
	local count_suffix=""
	local failures=""

	case "$test_cmd" in
		pytest*)
			passed=$(grep -Eo '[0-9]+ passed' "$output_file" | tail -n 1 | awk '{print $1}')
			failed=$(grep -Eo '[0-9]+ failed' "$output_file" | tail -n 1 | awk '{print $1}')
			if [ -n "$passed" ] || [ -n "$failed" ]; then
				local p=${passed:-0}
				local f=${failed:-0}
				total=$((p + f))
				if [ "$total" -gt 0 ]; then
					if [ "$test_status" = "pass" ]; then
						count_suffix=" ($passed/$total)"
					else
						count_suffix=" ($failed/$total)"
					fi
				fi
			fi
			if [ "$test_status" = "fail" ]; then
				failures=$(grep '^FAILED ' "$output_file" | sed 's/^FAILED //' | head -n 10)
			fi
			;;
		"npm test"*|"npm test --silent"*|"yarn test"*|"yarn test --silent"*|"pnpm test"*|"pnpm test --silent"*)
			# Jest and many JS runners print a "Tests:" summary; best-effort parse.
			# Example: "Tests:       2 failed, 3 passed, 5 total"
			local tests_line
			tests_line=$(grep -E '^Tests:' "$output_file" | tail -n 1 || true)
			if [ -n "$tests_line" ]; then
				failed=$(printf '%s\n' "$tests_line" | grep -Eo '[0-9]+ failed' | awk '{print $1}' | tail -n 1)
				passed=$(printf '%s\n' "$tests_line" | grep -Eo '[0-9]+ passed' | awk '{print $1}' | tail -n 1)
				total=$(printf '%s\n' "$tests_line" | grep -Eo '[0-9]+ total' | awk '{print $1}' | tail -n 1)
				if [ -n "$total" ]; then
					if [ "$test_status" = "pass" ] && [ -n "$passed" ]; then
						count_suffix=" ($passed/$total)"
					elif [ "$test_status" = "fail" ] && [ -n "$failed" ]; then
						count_suffix=" ($failed/$total)"
					fi
				fi
			fi
			if [ "$test_status" = "fail" ]; then
				# Jest prints failing suites as: "FAIL  path/to/test"
				failures=$(grep -E '^FAIL\s+' "$output_file" | sed -E 's/^FAIL\s+//' | head -n 10)
			fi
			;;
		dotnet\ test*)
			# Common dotnet summaries:
			#   Total tests: 12. Passed: 11. Failed: 1. Skipped: 0.
			#   Passed!  - Failed: 0, Passed: 12, Skipped: 0, Total: 12, Duration: ...
			local totals
			totals=$(grep -E 'Total tests:[[:space:]]*[0-9]+' "$output_file" | tail -n 1 || true)
			if [ -z "${totals// /}" ]; then
				totals=$(grep -E 'Failed:[[:space:]]*[0-9]+, Passed:[[:space:]]*[0-9]+, Skipped:[[:space:]]*[0-9]+, Total:[[:space:]]*[0-9]+' "$output_file" | tail -n 1 || true)
			fi
			if [ -n "${totals// /}" ]; then
				local t
				local p
				local f
				t=$(printf '%s\n' "$totals" | { grep -Eo 'Total tests:[[:space:]]*[0-9]+' || true; } | grep -Eo '[0-9]+' | tail -n 1 || true)
				if [ -z "${t// /}" ]; then
					t=$(printf '%s\n' "$totals" | { grep -Eo 'Total:[[:space:]]*[0-9]+' || true; } | grep -Eo '[0-9]+' | tail -n 1 || true)
				fi
				p=$(printf '%s\n' "$totals" | { grep -Eo 'Passed:[[:space:]]*[0-9]+' || true; } | grep -Eo '[0-9]+' | tail -n 1 || true)
				f=$(printf '%s\n' "$totals" | { grep -Eo 'Failed:[[:space:]]*[0-9]+' || true; } | grep -Eo '[0-9]+' | tail -n 1 || true)
				if [ -n "${t// /}" ]; then
					if [ "$test_status" = "pass" ] && [ -n "${p// /}" ]; then
						count_suffix=" ($p/$t)"
					elif [ "$test_status" = "fail" ] && [ -n "${f// /}" ]; then
						count_suffix=" ($f/$t)"
					fi
				fi
			fi

			if [ "$test_status" = "fail" ]; then
				# Best-effort: pair "Failed <TestName>" with the first following
				# message line (xUnit commonly prints:
				#   Failed <TestName>
				#   Error Message:
				#     <message>
				# ...). Avoid emitting a blank "Message:" bullet.
				failures=$(awk '
					function trim(s) { sub(/^[[:space:]]+/, "", s); sub(/[[:space:]]+$/, "", s); return s }
					function emit(t, m) {
						if (t == "") return
						if (m == "") { print t; return }
						print t " — " m
					}
					BEGIN { current=""; msg=""; want_msg=0; emitted=0; count=0 }
					/^[[:space:]]*Failed[[:space:]]+/ {
						# Emit previous failure if we never got a message.
						if (current != "" && !emitted) { emit(current, msg); count++; }
						if (count >= 10) exit
						current=$0
						sub(/^[[:space:]]*Failed[[:space:]]+/, "", current)
						current=trim(current)
						msg=""; want_msg=0; emitted=0
						next
					}
					{
						if (current == "") next
						if ($0 ~ /^[[:space:]]*(Error Message:|Message:)[[:space:]]*$/) { want_msg=1; next }
						if (want_msg) {
							if ($0 ~ /^[[:space:]]*$/) next
							msg=$0
							msg=trim(msg)
							emit(current, msg)
							emitted=1
							count++
							if (count >= 10) exit
							current=""; msg=""; want_msg=0
							next
						}
					}
					END {
						if (count < 10 && current != "" && !emitted) emit(current, msg)
					}
				' "$output_file")
			fi
			;;
		*)
			# Unknown runner; we will attempt generic parsing below.
			;;
	esac

	# Generic parsing fallback for wrappers like `make test`.
	# Only compute counts if we don't already have a suffix.
	if [ -z "${count_suffix// /}" ]; then
		# pytest-style summary often appears even when invoked via a wrapper.
		passed=$(grep -Eo '[0-9]+ passed' "$output_file" | tail -n 1 | awk '{print $1}' || true)
		failed=$(grep -Eo '[0-9]+ failed' "$output_file" | tail -n 1 | awk '{print $1}' || true)
		if [ -n "${passed// /}" ] || [ -n "${failed// /}" ]; then
			local p=${passed:-0}
			local f=${failed:-0}
			total=$((p + f))
			if [ "$total" -gt 0 ]; then
				if [ "$test_status" = "pass" ]; then
					count_suffix=" ($p/$total)"
				else
					count_suffix=" ($f/$total)"
				fi
			fi
		fi

		# Python unittest summary:
		#   Ran 48 tests in 0.123s
		#   OK
		# or
		#   FAILED (failures=1, errors=0)
		if [ -z "${count_suffix// /}" ]; then
			total=$(grep -E '^Ran[[:space:]]+[0-9]+[[:space:]]+tests?' "$output_file" | tail -n 1 | grep -Eo '[0-9]+' | tail -n 1 || true)
			if [ -n "${total// /}" ]; then
				if [ "$test_status" = "pass" ]; then
					count_suffix=" ($total/$total)"
				else
					# Best-effort failures count if present.
					local uf
					local ue
					uf=$(grep -Eo 'failures=[0-9]+' "$output_file" | tail -n 1 | grep -Eo '[0-9]+' | tail -n 1 || true)
					ue=$(grep -Eo 'errors=[0-9]+' "$output_file" | tail -n 1 | grep -Eo '[0-9]+' | tail -n 1 || true)
					uf=${uf:-0}
					ue=${ue:-0}
					failed=$((uf + ue))
					count_suffix=" ($failed/$total)"
				fi
			fi
		fi

		# Rust cargo test summary:
		#   test result: ok. 48 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out
		#   test result: FAILED. 47 passed; 1 failed; ...
		if [ -z "${count_suffix// /}" ]; then
			local cargo_line
			cargo_line=$(grep -E 'test result:[[:space:]]*(ok|FAILED)\.' "$output_file" | tail -n 1 || true)
			if [ -n "${cargo_line// /}" ]; then
				local cp
				local cf
				cp=$(printf '%s\n' "$cargo_line" | grep -Eo '[0-9]+ passed' | tail -n 1 | awk '{print $1}' || true)
				cf=$(printf '%s\n' "$cargo_line" | grep -Eo '[0-9]+ failed' | tail -n 1 | awk '{print $1}' || true)
				cp=${cp:-0}
				cf=${cf:-0}
				total=$((cp + cf))
				if [ "$total" -gt 0 ]; then
					if [ "$test_status" = "pass" ]; then
						count_suffix=" ($cp/$total)"
					else
						count_suffix=" ($cf/$total)"
					fi
				fi
			fi
		fi

		# Maven Surefire:
		#   Tests run: 48, Failures: 1, Errors: 0, Skipped: 0
		if [ -z "${count_suffix// /}" ]; then
			local mvn
			mvn=$(grep -E 'Tests run:[[:space:]]*[0-9]+,[[:space:]]*Failures:[[:space:]]*[0-9]+,[[:space:]]*Errors:[[:space:]]*[0-9]+,[[:space:]]*Skipped:[[:space:]]*[0-9]+' "$output_file" | tail -n 1 || true)
			if [ -n "${mvn// /}" ]; then
				local tr
				local tf
				local te
				tr=$(printf '%s\n' "$mvn" | grep -Eo 'Tests run:[[:space:]]*[0-9]+' | grep -Eo '[0-9]+' | tail -n 1 || true)
				tf=$(printf '%s\n' "$mvn" | grep -Eo 'Failures:[[:space:]]*[0-9]+' | grep -Eo '[0-9]+' | tail -n 1 || true)
				te=$(printf '%s\n' "$mvn" | grep -Eo 'Errors:[[:space:]]*[0-9]+' | grep -Eo '[0-9]+' | tail -n 1 || true)
				tr=${tr:-0}
				tf=${tf:-0}
				te=${te:-0}
				failed=$((tf + te))
				if [ "$tr" -gt 0 ]; then
					if [ "$test_status" = "pass" ]; then
						count_suffix=" ($tr/$tr)"
					else
						count_suffix=" ($failed/$tr)"
					fi
				fi
			fi
		fi

		# Gradle:
		#   48 tests completed, 1 failed
		if [ -z "${count_suffix// /}" ]; then
			local gradle
			gradle=$(grep -E '[0-9]+ tests completed, [0-9]+ failed' "$output_file" | tail -n 1 || true)
			if [ -n "${gradle// /}" ]; then
				total=$(printf '%s\n' "$gradle" | awk '{print $1}')
				failed=$(printf '%s\n' "$gradle" | awk '{print $4}')
				if [ -n "${total// /}" ] && [ -n "${failed// /}" ]; then
					if [ "$test_status" = "pass" ]; then
						count_suffix=" ($total/$total)"
					else
						count_suffix=" ($failed/$total)"
					fi
				fi
			fi
		fi
	fi

	local header="Testing: ${test_status}${count_suffix}"
	if [ "$test_status" = "fail" ] && [ -n "${failures// /}" ]; then
		printf '%s\n' "$header"
		printf '%s\n' "$failures" | sed 's/^/- /'
		return 0
	fi

	printf '%s' "$header"
}

normalize_ai_commit_message() {
	# Args:
	#  1) commit message (may be multi-line)
	#  2) authoritative testing line (single line)
	#  3) breaking hints (multi-line; optional)
	#  4) pre-computed risk level (low|medium|high)
	#  5) pre-computed risk reason
	local msg="$1"
	local testing_line="$2"
	local breaking_hints="$3"
	local computed_risk="${4:-}"
	local risk_reason="${5:-}"

	local tmp_bullets
	tmp_bullets=$(mktemp -t git-upload-testing-bullets.XXXXXX)
	# Extract bullet lines from the authoritative testing section.
	printf '%s\n' "$testing_line" | grep '^- ' >"$tmp_bullets" 2>/dev/null || true

	local has_likely_breaking=0
	if printf '%s\n' "$breaking_hints" | grep -qi 'LIKELY BREAKING'; then
		has_likely_breaking=1
	fi
	
	# Determine test impact for breaking change logic
	local testing_degraded=0
	if printf '%s\n' "$testing_line" | grep -q '^Testing: fail (degraded)'; then
		testing_degraded=1
	fi

	# 1) Drop any AI-provided Testing lines.
	# 2) Drop duplicated failure bullets outside Testing.
	# 3) Normalize Breaking changes yes/no.
	local normalized
	normalized=$(printf '%s\n' "$msg" \
		| grep -v '^Testing:' \
		| grep -vFf "$tmp_bullets" 2>/dev/null \
		| awk -v has_likely="$has_likely_breaking" -v testing_degraded="$testing_degraded" '
			function ltrim(s) { sub(/^[[:space:]]+/, "", s); return s }
			function rtrim(s) { sub(/[[:space:]]+$/, "", s); return s }
			function trim(s) { return rtrim(ltrim(s)) }
			BEGIN { rewrote=0; in_breaking=0 }
			{
				line=$0

				if (line == "Breaking changes:") {
					in_breaking=1
					print line
					next
				}
				# If we leave the breaking section (blank line or new section header), stop de-hedging.
				if (in_breaking==1 && (line ~ /^[[:space:]]*$/ || line ~ /^[A-Z][A-Za-z ]+:[[:space:]]*$/)) {
					in_breaking=0
				}

				if (match(line, /^Breaking changes:[[:space:]]*/)) {
					rest=line
					sub(/^Breaking changes:[[:space:]]*/, "", rest)
					rest=trim(rest)

					low=tolower(rest)
					# yes/no normalization
					split(rest, parts, /[[:space:]]+/)
					yn=tolower(parts[1])
					if (!rewrote && (yn=="yes" || yn=="no")) {
						rewrote=1
						body=rest
						# drop the first word (yes/no) without relying on non-portable regex flags
						sub(/^[^[:space:]]+[[:space:]]*/, "", body)
						body=trim(body)
						sub(/^[-–—:]+[[:space:]]*/, "", body)
						if (yn=="no") {
							print "Breaking changes: none"
							next
						}
						print "Breaking changes:"
						if (length(body) > 0) print "- " body
						else print "- (details missing; review staged diff)"
						next
					}

					# Only force breaking changes if we have STRONG evidence (API removal, test degradation)
					if (low=="none" && (has_likely==1 || testing_degraded==1)) {
						print "Breaking changes:"
						print "- Likely breaking change detected; review staged diff"
						next
					}
				}
				# De-hedge bullets only when we have strong signals (test degradation)
				if (in_breaking==1 && (has_likely==1 || testing_degraded==1) && line ~ /^- /) {
					gsub(/ may now /, " will now ", line)
					gsub(/ might now /, " will now ", line)
					gsub(/ may no longer /, " will no longer ", line)
					gsub(/ might no longer /, " will no longer ", line)
					gsub(/ may be /, " is ", line)
					gsub(/ might be /, " is ", line)
					gsub(/ may /, " will ", line)
					gsub(/ might /, " will ", line)
				}
				print line
			}
		')

	rm -f "$tmp_bullets" >/dev/null 2>&1 || true

	# Append authoritative Testing section.
	if [ -n "${testing_line// /}" ]; then
		normalized="$normalized

$testing_line"
	fi

	# Determine whether the final message indicates breaking changes (bullet list).
	local breaking_kind
	breaking_kind=$(printf '%s\n' "$normalized" | awk '
		BEGIN{kind="none"}
		/^Breaking changes:[[:space:]]*none[[:space:]]*$/ { kind="none" }
		/^Breaking changes:[[:space:]]*$/ { kind="bullets" }
		END{ print kind }
	')

	# Use pre-computed risk - it's based on contextualized impact analysis
	# Only override if we detect breaking API changes with bullet list
	local final_risk="$computed_risk"
	local final_reason="$risk_reason"
	
	# Override: explicit breaking changes with likely API removal -> high
	if [ "$breaking_kind" = "bullets" ] && [ "$has_likely_breaking" -eq 1 ]; then
		final_risk="high"
		final_reason="breaking API changes detected"
	fi
	
	# Default fallback
	if [ -z "$final_risk" ]; then
		final_risk="medium"
		final_reason="review recommended"
	fi

	# Replace or append risk line with the computed risk
	if printf '%s\n' "$normalized" | grep -q '^Risk:'; then
		normalized=$(printf '%s\n' "$normalized" | awk -v risk="$final_risk" -v reason="$final_reason" '
			BEGIN{done=0}
			/^Risk:/ {
				if (!done) {
					print "Risk: " risk " (" reason ")"
					done=1
					next
				}
			}
			{ print }
		')
	else
		normalized="$normalized

Risk: $final_risk ($final_reason)"
	fi

	# Trim trailing blank lines.
	normalized=$(printf '%s\n' "$normalized" | awk '{ lines[NR]=$0 } $0 !~ /^[[:space:]]*$/ { last=NR } END { for (i=1; i<=last; i++) print lines[i] }')
	printf '%s' "$normalized"
}

compute_diff_analysis() {
	# Comprehensive diff analysis that determines ACTUAL risk based on:
	# 1. What type of changes are being made (code vs formatting vs comments)
	# 2. The semantic impact of changes (new vs modified vs deleted functionality)
	# 3. Whether changes introduce potential issues (dead code, syntax errors)
	# 4. Test results and syntax validation
	#
	# Returns a structured analysis that can be used by AI and risk scoring.
	
	local diff
	diff=$(git diff --cached 2>/dev/null || echo "")
	if [ -z "${diff// /}" ]; then
		printf 'diff_empty=1\n'
		return 0
	fi

	# Count actual code changes vs whitespace/formatting
	local total_additions=0
	local total_deletions=0
	local whitespace_only_changes=0
	local comment_only_changes=0
	local code_additions=0
	local code_deletions=0
	local files_changed=0
	local test_files_changed=0
	local config_files_changed=0
	local core_files_changed=0
	
	# Get list of changed files
	local changed_files
	changed_files=$(git diff --cached --name-only 2>/dev/null || echo "")
	files_changed=$(printf '%s\n' "$changed_files" | grep -c . || echo 0)
	
	# Categorize changed files
	while IFS= read -r file; do
		[ -z "$file" ] && continue
		case "$file" in
			*test*|*Test*|*spec*|*Spec*|*_test.*|*.test.*)
				test_files_changed=$((test_files_changed + 1))
				;;
			*.json|*.yaml|*.yml|*.toml|*.ini|*.cfg|*.conf|*config*|*.env*)
				config_files_changed=$((config_files_changed + 1))
				;;
			*)
				core_files_changed=$((core_files_changed + 1))
				;;
		esac
	done <<< "$changed_files"
	
	# Analyze the actual diff content
	local in_hunk=0
	local current_file=""
	while IFS= read -r line; do
		case "$line" in
			"diff --git"*)
				current_file=$(printf '%s' "$line" | sed 's/.*b\///')
				;;
			"+"[!+]*)
				total_additions=$((total_additions + 1))
				# Check if this is whitespace-only
				local content="${line#?}"
				if [ -z "${content// /}" ] || [ -z "${content//	/}" ]; then
					whitespace_only_changes=$((whitespace_only_changes + 1))
				# Check if this is a comment
				elif printf '%s' "$content" | grep -qE '^[[:space:]]*(//|#|/\*|\*|<!--)'; then
					comment_only_changes=$((comment_only_changes + 1))
				else
					code_additions=$((code_additions + 1))
				fi
				;;
			"-"[!-]*)
				total_deletions=$((total_deletions + 1))
				local content="${line#?}"
				if [ -z "${content// /}" ] || [ -z "${content//	/}" ]; then
					whitespace_only_changes=$((whitespace_only_changes + 1))
				elif printf '%s' "$content" | grep -qE '^[[:space:]]*(//|#|/\*|\*|<!--)'; then
					comment_only_changes=$((comment_only_changes + 1))
				else
					code_deletions=$((code_deletions + 1))
				fi
				;;
		esac
	done <<< "$diff"
	
	# Check for potential dead code patterns in additions
	local dead_code_signals=0
	local dead_code_details=""
	# Unreachable code after return/exit
	if printf '%s\n' "$diff" | grep -qE '^\+.*return[[:space:]]*;' && \
	   printf '%s\n' "$diff" | grep -qE '^\+[^}]*[^/].*[^/]$' 2>/dev/null; then
		# Very rough heuristic - look for code after returns that isn't closing braces
		:
	fi
	# Unused imports (added imports that might not be used)
	local added_imports
	added_imports=$(printf '%s\n' "$diff" | grep -E '^\+.*(import |from .* import |require\(|#include)' | wc -l | tr -d ' ')
	
	# Check for syntax errors in changed files (language-specific)
	local syntax_errors=""
	local syntax_error_count=0
	while IFS= read -r file; do
		[ -z "$file" ] && continue
		[ ! -f "$file" ] && continue
		
		case "$file" in
			*.py)
				if command -v python3 >/dev/null 2>&1; then
					if ! python3 -m py_compile "$file" >/dev/null 2>&1; then
						syntax_errors="${syntax_errors}
- Python syntax error in $file"
						syntax_error_count=$((syntax_error_count + 1))
					fi
				fi
				;;
			*.js|*.jsx)
				# Only check plain JavaScript files with node --check
				if command -v node >/dev/null 2>&1; then
					if ! node --check "$file" >/dev/null 2>&1; then
						syntax_errors="${syntax_errors}
- JavaScript syntax error in $file"
						syntax_error_count=$((syntax_error_count + 1))
					fi
				fi
				;;
			*.ts|*.tsx)
				# TypeScript requires tsc for proper checking, skip if not available
				if command -v tsc >/dev/null 2>&1; then
					if ! tsc --noEmit "$file" >/dev/null 2>&1; then
						syntax_errors="${syntax_errors}
- TypeScript syntax error in $file"
						syntax_error_count=$((syntax_error_count + 1))
					fi
				fi
				;;
			*.sh|*.bash|*.zsh)
				if command -v bash >/dev/null 2>&1; then
					if ! bash -n "$file" >/dev/null 2>&1; then
						syntax_errors="${syntax_errors}
- Shell syntax error in $file"
						syntax_error_count=$((syntax_error_count + 1))
					fi
				fi
				;;
			*.json)
				if command -v python3 >/dev/null 2>&1; then
					if ! python3 -c "import json; json.load(open('$file'))" >/dev/null 2>&1; then
						syntax_errors="${syntax_errors}
- JSON syntax error in $file"
						syntax_error_count=$((syntax_error_count + 1))
					fi
				fi
				;;
		esac
	done <<< "$changed_files"
	
	# Check for removed public APIs/exports (actual breaking changes)
	local api_removals=0
	local api_removal_details=""
	# Look for removed function/class/export declarations
	local removed_exports
	removed_exports=$(printf '%s\n' "$diff" | grep -E '^-.*(^export |^public |^def |^function |^class |^module )' | head -n 5 || true)
	if [ -n "${removed_exports// /}" ]; then
		api_removals=$(printf '%s\n' "$removed_exports" | wc -l | tr -d ' ')
		api_removal_details="$removed_exports"
	fi
	
	# Check for changed function signatures (potential breaking)
	local signature_changes=0
	# This is complex - for now, just flag if function definitions changed
	local modified_signatures
	modified_signatures=$(printf '%s\n' "$diff" | grep -E '^[-+].*(def |function |func |fn )[a-zA-Z_][a-zA-Z0-9_]*\(' | wc -l | tr -d ' ')
	if [ "$modified_signatures" -gt 0 ]; then
		signature_changes=$((modified_signatures / 2))  # Roughly - a change has both - and +
	fi
	
	# Output structured analysis
	printf 'diff_empty=0\n'
	printf 'files_changed=%d\n' "$files_changed"
	printf 'test_files_changed=%d\n' "$test_files_changed"
	printf 'config_files_changed=%d\n' "$config_files_changed"
	printf 'core_files_changed=%d\n' "$core_files_changed"
	printf 'total_additions=%d\n' "$total_additions"
	printf 'total_deletions=%d\n' "$total_deletions"
	printf 'code_additions=%d\n' "$code_additions"
	printf 'code_deletions=%d\n' "$code_deletions"
	printf 'whitespace_changes=%d\n' "$whitespace_only_changes"
	printf 'comment_changes=%d\n' "$comment_only_changes"
	printf 'syntax_error_count=%d\n' "$syntax_error_count"
	printf 'api_removals=%d\n' "$api_removals"
	printf 'signature_changes=%d\n' "$signature_changes"
	if [ -n "$syntax_errors" ]; then
		printf 'syntax_errors=%s\n' "$syntax_errors"
	fi
	if [ -n "$api_removal_details" ]; then
		printf 'api_removal_details=%s\n' "$api_removal_details"
	fi
}

compute_risk_score() {
	# Risk assessment based on CONTEXTUALIZED IMPACT of the changes:
	#
	# LOW RISK: Changes that improve the codebase or have minimal impact
	#   - Fewer test failures than before (fixing things)
	#   - All tests pass
	#   - Changes that don't introduce new problems
	#
	# MEDIUM RISK: Changes with uncertain impact that need review/testing
	#   - Same number of test failures (neutral)
	#   - Unknown baseline comparison
	#   - New functionality that hasn't been tested yet
	#
	# HIGH RISK: Changes that make things worse or introduce vulnerabilities
	#   - More test failures than before (breaking things)
	#   - Tests that were passing now fail
	#   - Syntax errors introduced
	#   - Removal of public APIs (could break consumers)
	#
	# Returns: low, medium, or high with a reason
	
	local testing_status="$1"
	local diff_analysis="$2"
	
	# Parse diff analysis
	local diff_empty=0
	local files_changed=0
	local test_files_changed=0
	local config_files_changed=0
	local core_files_changed=0
	local total_additions=0
	local total_deletions=0
	local code_additions=0
	local code_deletions=0
	local whitespace_changes=0
	local comment_changes=0
	local syntax_error_count=0
	local api_removals=0
	local signature_changes=0
	
	eval "$diff_analysis"
	
	local risk_level="medium"  # Default to medium (unknown/uncertain)
	local risk_reasons=""
	
	# =================================================================
	# HIGH RISK: Changes that actively make things WORSE
	# =================================================================
	
	# 1. Tests degraded (more failures, or tests broke that were passing)
	if printf '%s\n' "$testing_status" | grep -q '^Testing: fail (degraded)'; then
		risk_level="high"
		risk_reasons="commit degraded test health"
	fi
	
	# 2. Syntax errors introduced
	if [ "$syntax_error_count" -gt 0 ]; then
		risk_level="high"
		if [ -n "$risk_reasons" ]; then
			risk_reasons="$risk_reasons, syntax errors detected"
		else
			risk_reasons="syntax errors detected"
		fi
	fi
	
	# 3. Public API/exports removed (could break downstream consumers)
	if [ "$api_removals" -gt 0 ]; then
		risk_level="high"
		if [ -n "$risk_reasons" ]; then
			risk_reasons="$risk_reasons, API exports removed"
		else
			risk_reasons="API exports removed (potential breaking change)"
		fi
	fi
	
	# =================================================================
	# LOW RISK: Changes that IMPROVE things or have minimal impact
	# =================================================================
	
	# 4. Tests improving (fewer failures than before) - this commit is FIXING things
	if printf '%s\n' "$testing_status" | grep -q '^Testing: fail (improving)'; then
		if [ "$risk_level" != "high" ]; then
			risk_level="low"
			risk_reasons="commit is fixing test failures"
		fi
	fi
	
	# 5. All tests pass - codebase is healthy
	if printf '%s\n' "$testing_status" | grep -q '^Testing: pass'; then
		if [ "$risk_level" != "high" ]; then
			risk_level="low"
			risk_reasons="all tests pass"
		fi
	fi
	
	# 6. No tests configured but no syntax errors or API removals
	if printf '%s\n' "$testing_status" | grep -q '^Testing: not configured'; then
		if [ "$risk_level" != "high" ]; then
			# Without tests, we rely on other signals - default to medium for untested changes
			if [ "$risk_level" != "low" ]; then
				risk_level="medium"
				risk_reasons="changes not tested (no test suite)"
			fi
		fi
	fi
	
	# =================================================================
	# MEDIUM RISK: Uncertain impact, needs review
	# =================================================================
	
	# 7. Same number of failures (neutral impact)
	if printf '%s\n' "$testing_status" | grep -q '^Testing: fail (unchanged)'; then
		if [ "$risk_level" != "high" ] && [ "$risk_level" != "low" ]; then
			risk_level="medium"
			risk_reasons="test failures unchanged (neutral impact)"
		fi
	fi
	
	# 8. Unknown baseline (couldn't compare)
	if printf '%s\n' "$testing_status" | grep -q '^Testing: fail (unknown baseline)'; then
		if [ "$risk_level" != "high" ] && [ "$risk_level" != "low" ]; then
			risk_level="medium"
			risk_reasons="could not compare against baseline"
		fi
	fi
	
	# 9. Function signatures changed (could affect callers, but not certain)
	if [ "$risk_level" = "low" ] && [ "$signature_changes" -gt 0 ]; then
		risk_level="medium"
		risk_reasons="function signatures modified (review callers)"
	fi
	
	# Default reason if none set
	if [ -z "$risk_reasons" ]; then
		case "$risk_level" in
			low)
				risk_reasons="routine changes"
				;;
			medium)
				risk_reasons="review recommended"
				;;
			high)
				risk_reasons="significant impact detected"
				;;
		esac
	fi
	
	printf '%s|%s' "$risk_level" "$risk_reasons"
}

compute_breaking_change_hints() {
	# Smarter breaking change detection based on actual semantic impact,
	# not just keyword matching.
	
	local diff
	diff=$(git diff --cached -U3 2>/dev/null || echo "")
	if [ -z "${diff// /}" ]; then
		printf '%s' 'Breaking-change hints: no staged diff detected.'
		return 0
	fi

	local hints=""
	local has_breaking=0
	local has_potential=0
	
	# 1. Check for removed public APIs/exports (ACTUAL breaking changes)
	local removed_exports
	removed_exports=$(printf '%s\n' "$diff" | grep -E '^-[[:space:]]*(export|public|def |function |class |module\.exports)' | grep -Ev '^-\s*(//|#|/\*|\*)' | head -n 5 || true)
	if [ -n "${removed_exports// /}" ]; then
		has_breaking=1
		hints="$hints\nLikely breaking: Public API/export removed:\n$(printf '%s\n' "$removed_exports" | sed 's/^/  /')"
	fi
	
	# 2. Check for changed function/method signatures
	# Look for functions where parameters changed
	local removed_funcs
	local added_funcs
	removed_funcs=$(printf '%s\n' "$diff" | grep -E '^-[[:space:]]*(def|function|func|fn|public|private|protected)[[:space:]]+[a-zA-Z_]' | head -n 10 || true)
	added_funcs=$(printf '%s\n' "$diff" | grep -E '^\+[[:space:]]*(def|function|func|fn|public|private|protected)[[:space:]]+[a-zA-Z_]' | head -n 10 || true)
	
	if [ -n "${removed_funcs// /}" ] && [ -n "${added_funcs// /}" ]; then
		# Function definitions changed - could be signature change
		has_potential=1
		hints="$hints\nPotential breaking: Function definitions modified (check for signature changes)"
	fi
	
	# 3. Check for removed error handling (relaxed validation)
	local removed_validation
	removed_validation=$(printf '%s\n' "$diff" | grep -E '^-.*\b(throw|raise|assert|panic|error\(|Error\(|Exception)\b' | grep -Ev '^-\s*(//|#|/\*|\*)' | head -n 3 || true)
	local added_validation
	added_validation=$(printf '%s\n' "$diff" | grep -E '^\+.*\b(throw|raise|assert|panic|error\(|Error\(|Exception)\b' | grep -Ev '^\+\s*(//|#|/\*|\*)' | head -n 3 || true)
	
	if [ -n "${removed_validation// /}" ] && [ -z "${added_validation// /}" ]; then
		has_potential=1
		hints="$hints\nPotential breaking: Error handling/validation removed"
	fi
	
	# 4. Check for changed default values
	local changed_defaults
	changed_defaults=$(printf '%s\n' "$diff" | grep -E '^[-+].*=[[:space:]]*(true|false|null|nil|None|0|1|""|'"'"''"'"')' | wc -l | tr -d ' ')
	if [ "$changed_defaults" -gt 2 ]; then
		has_potential=1
		hints="$hints\nPotential: Default values modified"
	fi
	
	# 5. Do NOT flag simple keyword matches as breaking
	# The old logic flagged things like "deprecated" in comments - we don't do that anymore
	
	# Build final output
	if [ "$has_breaking" -eq 1 ]; then
		printf 'Breaking-change hints: LIKELY BREAKING changes detected:%s' "$hints"
	elif [ "$has_potential" -eq 1 ]; then
		printf 'Breaking-change hints: Potential behavior changes detected (review recommended):%s' "$hints"
	else
		printf '%s' 'Breaking-change hints: No obvious breaking-change patterns detected.'
	fi
}

compute_testing_status() {
	# Returns testing status with comparison against previous commit.
	# Format: "Testing: <status> [(<details>)]"
	# Where status can be:
	#   - "pass" - all tests pass
	#   - "fail (new)" - tests that were passing before now fail (THIS COMMIT BROKE THEM)
	#   - "fail (pre-existing)" - tests failing but they were already failing before this commit
	#   - "not configured" - no test runner detected
	
	local repo_root
	repo_root=$(git rev-parse --show-toplevel 2>/dev/null || echo "")
	if [ -z "${repo_root// /}" ]; then
		printf '%s' 'Testing: not configured'
		return 0
	fi

	local test_spec
	if ! test_spec=$(detect_test_cmd "$repo_root"); then
		printf '%s' 'Testing: not configured'
		return 0
	fi
	local test_cmd
	local test_cwd
	IFS=$'\n' read -r test_cmd test_cwd <<EOF
$test_spec
EOF
	if [ -z "${test_cmd// /}" ]; then
		printf '%s' 'Testing: not configured'
		return 0
	fi
	if [ -z "${test_cwd// /}" ]; then
		test_cwd="$repo_root"
	fi

	# First, run tests on current state (with staged changes)
	echo "[git-upload] Running tests on current changes..." >&2

	local tmp_current
	tmp_current=$(mktemp -t git-upload-tests-current.XXXXXX)
	local current_exit_code=0

	if (cd "$test_cwd" && eval "$test_cmd") >"$tmp_current" 2>&1; then
		current_exit_code=0
	else
		current_exit_code=$?
	fi

	# Check for "no tests" conditions first
	if [ "$current_exit_code" -ne 0 ]; then
		# Pytest: exit 5, plus common text (works even if invoked via wrappers).
		if grep -qiE 'collected[[:space:]]+0[[:space:]]+items|no[[:space:]]+tests[[:space:]]+ran' "$tmp_current" 2>/dev/null; then
			rm -f "$tmp_current" >/dev/null 2>&1 || true
			echo "[git-upload] Testing: not configured" >&2
			printf '%s' 'Testing: not configured'
			return 0
		fi

		# JS runners (Jest/Vitest/Mocha wrappers): "No tests found" variants.
		if grep -qiE 'no[[:space:]]+tests[[:space:]]+found|no[[:space:]]+test[[:space:]]+files[[:space:]]+found|no[[:space:]]+tests[[:space:]]+to[[:space:]]+run' "$tmp_current" 2>/dev/null; then
			rm -f "$tmp_current" >/dev/null 2>&1 || true
			echo "[git-upload] Testing: not configured" >&2
			printf '%s' 'Testing: not configured'
			return 0
		fi

		# Maven/Surefire can be configured to fail when no tests match.
		if grep -qiE 'No tests were executed|There are no tests to run|No tests to run' "$tmp_current" 2>/dev/null; then
			rm -f "$tmp_current" >/dev/null 2>&1 || true
			echo "[git-upload] Testing: not configured" >&2
			printf '%s' 'Testing: not configured'
			return 0
		fi

		# dotnet test can fail when it discovers zero tests.
		if grep -qiE 'No test is available|No tests are available|No test files were found|No test matches the given testcase filter' "$tmp_current" 2>/dev/null; then
			rm -f "$tmp_current" >/dev/null 2>&1 || true
			echo "[git-upload] Testing: not configured" >&2
			printf '%s' 'Testing: not configured'
			return 0
		fi
	fi

	# Treat "no tests collected" as not configured (common for pytest).
	if printf '%s' "$test_cmd" | grep -q '^pytest' && [ "$current_exit_code" -eq 5 ]; then
		rm -f "$tmp_current" >/dev/null 2>&1 || true
		echo "[git-upload] Testing: not configured" >&2
		printf '%s' 'Testing: not configured'
		return 0
	fi

	# Treat JS package managers' default placeholder tests as not configured.
	if printf '%s' "$test_cmd" | grep -Eq '^(npm|yarn|pnpm) test' && grep -qi 'no test specified' "$tmp_current" 2>/dev/null; then
		rm -f "$tmp_current" >/dev/null 2>&1 || true
		echo "[git-upload] Testing: not configured" >&2
		printf '%s' 'Testing: not configured'
		return 0
	fi

	# If current tests pass, we're done - no need to check baseline
	if [ "$current_exit_code" -eq 0 ]; then
		local summary
		if summary=$(summarize_test_output "$test_cmd" "$current_exit_code" "$tmp_current" 2>/dev/null); then
			:
		else
			summary='Testing: pass'
		fi
		rm -f "$tmp_current" >/dev/null 2>&1 || true
		echo "[git-upload] $summary" >&2
		printf '%s' "$summary"
		return 0
	fi

	# Tests are failing - now we need to compare against baseline
	# to determine if this commit made things BETTER, WORSE, or SAME
	
	# Extract failure count from current test output
	local current_fail_count
	current_fail_count=$(extract_test_failure_count "$tmp_current")
	
	# Check if we have a previous commit to compare against
	local has_head=0
	if git rev-parse HEAD >/dev/null 2>&1; then
		has_head=1
	fi
	
	local test_delta="unknown"  # worse, same, better, or unknown
	local baseline_fail_count=0
	
	if [ "$has_head" -eq 1 ]; then
		echo "[git-upload] Tests failing ($current_fail_count) - comparing against previous commit..." >&2
		
		# Stash current changes (including staged), run tests on HEAD, then restore
		local stash_result
		stash_result=$(git stash push -u -m "git-upload-baseline-test" 2>&1 || echo "stash_failed")
		
		if printf '%s' "$stash_result" | grep -qv "stash_failed"; then
			local tmp_baseline
			tmp_baseline=$(mktemp -t git-upload-tests-baseline.XXXXXX)
			
			echo "[git-upload] Running tests on previous commit (HEAD) for comparison..." >&2
			
			local baseline_exit_code=0
			if (cd "$test_cwd" && eval "$test_cmd") >"$tmp_baseline" 2>&1; then
				baseline_exit_code=0
			else
				baseline_exit_code=$?
			fi
			
			# Restore the stashed changes
			git stash pop >/dev/null 2>&1 || true
			
			# Extract baseline failure count
			if [ "$baseline_exit_code" -eq 0 ]; then
				baseline_fail_count=0
			else
				baseline_fail_count=$(extract_test_failure_count "$tmp_baseline")
			fi
			
			# Compare: did this commit make things better, worse, or same?
			if [ "$baseline_fail_count" -eq 0 ] && [ "$current_fail_count" -gt 0 ]; then
				# Tests WERE passing, now failing = WORSE (this commit broke things)
				test_delta="worse"
				echo "[git-upload] Tests were passing (0 failures), now $current_fail_count failing - commit BROKE tests" >&2
			elif [ "$current_fail_count" -gt "$baseline_fail_count" ]; then
				# More failures now = WORSE
				test_delta="worse"
				echo "[git-upload] More tests failing now ($current_fail_count) than before ($baseline_fail_count) - commit made things WORSE" >&2
			elif [ "$current_fail_count" -lt "$baseline_fail_count" ]; then
				# Fewer failures now = BETTER (commit is fixing things!)
				test_delta="better"
				echo "[git-upload] Fewer tests failing now ($current_fail_count) than before ($baseline_fail_count) - commit is IMPROVING things" >&2
			else
				# Same number of failures
				test_delta="same"
				echo "[git-upload] Same number of test failures ($current_fail_count) - neutral impact" >&2
			fi
			
			rm -f "$tmp_baseline" >/dev/null 2>&1 || true
		else
			# Couldn't stash - assume neutral to avoid false positives
			test_delta="unknown"
			echo "[git-upload] Could not compare against HEAD - impact unknown" >&2
		fi
	else
		# No previous commit - this is initial commit with failing tests
		test_delta="worse"
		echo "[git-upload] No previous commit - initial commit has $current_fail_count failing tests" >&2
	fi

	# Build the summary with delta information
	local summary
	if summary=$(summarize_test_output "$test_cmd" "$current_exit_code" "$tmp_current" 2>/dev/null); then
		:
	else
		summary='Testing: fail'
	fi
	rm -f "$tmp_current" >/dev/null 2>&1 || true

	# Modify the summary to indicate the impact of this commit on test health
	case "$test_delta" in
		worse)
			summary=$(printf '%s' "$summary" | sed 's/^Testing: fail/Testing: fail (degraded)/')
			;;
		better)
			summary=$(printf '%s' "$summary" | sed 's/^Testing: fail/Testing: fail (improving)/')
			;;
		same)
			summary=$(printf '%s' "$summary" | sed 's/^Testing: fail/Testing: fail (unchanged)/')
			;;
		*)
			summary=$(printf '%s' "$summary" | sed 's/^Testing: fail/Testing: fail (unknown baseline)/')
			;;
	esac

	if [ -z "${summary// /}" ]; then
		summary='Testing: not configured'
	fi

	echo "[git-upload] $summary" >&2
	printf '%s' "$summary"
}

extract_test_failure_count() {
	# Best-effort extraction of failure count from test output
	# Returns a number (0 if can't determine)
	local output_file="$1"
	local count=0
	
	# Try various patterns for different test runners
	
	# Jest/Vitest: "Tests: X failed"
	local jest_fail
	jest_fail=$(grep -oE 'Tests:[[:space:]]+[0-9]+[[:space:]]+failed' "$output_file" 2>/dev/null | grep -oE '[0-9]+' | head -n 1 || true)
	if [ -n "$jest_fail" ]; then
		printf '%s' "$jest_fail"
		return 0
	fi
	
	# Pytest: "X failed" or "failed: X"
	local pytest_fail
	pytest_fail=$(grep -oE '[0-9]+[[:space:]]+failed' "$output_file" 2>/dev/null | grep -oE '[0-9]+' | head -n 1 || true)
	if [ -n "$pytest_fail" ]; then
		printf '%s' "$pytest_fail"
		return 0
	fi
	
	# Go: "FAIL" lines (count them)
	local go_fail
	go_fail=$(grep -c '^--- FAIL:' "$output_file" 2>/dev/null || echo "0")
	if [ "$go_fail" -gt 0 ]; then
		printf '%s' "$go_fail"
		return 0
	fi
	
	# Maven: "Failures: X, Errors: Y"
	local mvn_fail
	mvn_fail=$(grep -oE 'Failures:[[:space:]]*[0-9]+' "$output_file" 2>/dev/null | grep -oE '[0-9]+' | tail -n 1 || true)
	local mvn_err
	mvn_err=$(grep -oE 'Errors:[[:space:]]*[0-9]+' "$output_file" 2>/dev/null | grep -oE '[0-9]+' | tail -n 1 || true)
	if [ -n "$mvn_fail" ] || [ -n "$mvn_err" ]; then
		mvn_fail=${mvn_fail:-0}
		mvn_err=${mvn_err:-0}
		printf '%s' "$((mvn_fail + mvn_err))"
		return 0
	fi
	
	# Cargo/Rust: "X failed"
	local cargo_fail
	cargo_fail=$(grep -oE '[0-9]+[[:space:]]+failed' "$output_file" 2>/dev/null | grep -oE '[0-9]+' | head -n 1 || true)
	if [ -n "$cargo_fail" ]; then
		printf '%s' "$cargo_fail"
		return 0
	fi
	
	# dotnet: "Failed: X"
	local dotnet_fail
	dotnet_fail=$(grep -oE 'Failed:[[:space:]]*[0-9]+' "$output_file" 2>/dev/null | grep -oE '[0-9]+' | head -n 1 || true)
	if [ -n "$dotnet_fail" ]; then
		printf '%s' "$dotnet_fail"
		return 0
	fi
	
	# Fallback: count lines with common failure indicators
	local generic_fail
	generic_fail=$(grep -ciE '(FAIL|FAILED|ERROR|BROKEN)' "$output_file" 2>/dev/null || echo "0")
	if [ "$generic_fail" -gt 0 ]; then
		printf '%s' "$generic_fail"
		return 0
	fi
	
	# Can't determine - return 1 as a fallback (at least 1 failure since tests failed)
	printf '1'
}

generate_ai_message() {
	local ai_cmd

	if [ -n "${GIT_UPLOAD_AI_CMD-}" ]; then
		ai_cmd="$GIT_UPLOAD_AI_CMD"
	else
		ai_cmd="$DEFAULT_AI_CMD"
	fi

	local ai_binary
	ai_binary=${ai_cmd%% *}
	if ! command -v "$ai_binary" >/dev/null 2>&1; then
		echo "[git-upload] AI command '$ai_binary' not found. " \
			"Install GitHub Copilot CLI (e.g. 'brew install copilot-cli' on macOS) " \
			"and ensure it is on your PATH." >&2
		return 1
	fi

	local testing_status
	testing_status=$(compute_testing_status)
	local breaking_hints
	breaking_hints=$(compute_breaking_change_hints)
	
	# Get comprehensive diff analysis for smart risk assessment
	local diff_analysis
	diff_analysis=$(compute_diff_analysis)
	
	# Compute actual risk based on meaningful criteria
	local risk_result
	risk_result=$(compute_risk_score "$testing_status" "$diff_analysis")
	local computed_risk="${risk_result%%|*}"
	local risk_reason="${risk_result#*|}"
	
	# Build diff summary for AI context
	local diff_summary=""
	eval "$diff_analysis"
	diff_summary="Diff analysis:
- Files changed: $files_changed (core: $core_files_changed, tests: $test_files_changed, config: $config_files_changed)
- Code changes: +$code_additions/-$code_deletions lines
- Formatting/whitespace changes: $whitespace_changes lines
- Comment changes: $comment_changes lines
- Syntax errors detected: $syntax_error_count
- API/export removals: $api_removals
- Function signature changes: $signature_changes

Pre-computed risk assessment: $computed_risk ($risk_reason)"

	local default_prompt="You are generating a git commit message for the CURRENT STAGED CHANGES.

Your job:
- Determine what changed between the last commit (HEAD) and the current staged snapshot (index). If there is no HEAD yet (initial commit), base it on the staged snapshot.
- Write a reasonable-length but very informative commit message that explains what changed and why, so a teammate can understand it weeks later.

Content requirements:
- Base everything on the staged diff (index) vs HEAD; do not invent changes.
- If multiple areas changed, group them; prioritize user-visible behavior changes and operational impact.
- Call out CLI/API behavior changes, default behavior changes, and backwards compatibility.
- IMPORTANT: If behavior changed in a way that could affect users (including relaxing/tightening validation, changing parsing, changing defaults, changing outputs, removing checks/guards, changing error handling), treat it as a breaking change unless you can justify compatibility.
- If a change is clearly a bugfix that restores documented/spec-intended behavior, it may be non-breaking; still call out the behavior change in Summary/Why.
- Breaking changes format rules (STRICT):
	- If there are NO breaking changes, output exactly: Breaking changes: none
	- If there ARE breaking changes, output:
		Breaking changes:
		- <bullet 1>
		- <bullet 2>
		(Do NOT use Yes/No. Do NOT write 'Breaking changes: yes'.)

RISK ASSESSMENT POLICY (IMPORTANT - use the pre-computed risk as your baseline):
The risk level has been pre-computed based on ACTUAL analysis of the diff:
- Syntax errors in changed files
- Test failures
- Removed public APIs/exports
- Changed function signatures
- Ratio of code changes vs formatting/whitespace changes

Use the pre-computed risk level unless you have strong evidence to increase it.
DO NOT mark changes as high risk just because they contain keywords like 'validate' or 'throw'.
Formatting-only changes, comment changes, and test-only changes are LOW risk.
Changes that don't modify any actual code logic are LOW risk.

- For Testing, use the provided testing status EXACTLY (verbatim). Do not guess.
- If tests failed, do NOT restate the failing test list anywhere except under the Testing section.
- For breaking changes bullets, avoid hedging: do not use may or might; use definitive language.
- For breaking changes bullets, be concrete: name what breaks in accordance with the project goals.

Style requirements:
- Subject line: imperative mood; concise and specific; aim for <= 72 characters when feasible.
- Body: detailed but not padded; prefer bullets.
- Do NOT mention checking status, diffs, tools, agents, prompts, or instructions.

Output protocol (IMPORTANT):
- Output ONLY the commit message between these exact markers, with no other text:
COMMIT_BEGIN
<subject line>

<body, multiple lines allowed>
COMMIT_END

Suggested body structure (adapt as needed):
Summary:
- <bullet per major change>
Why:
- <motivation/problem addressed>
Breaking changes: <use the strict format rules above>
Risk: <low|medium|high> (<short rationale>)
Testing: <use the provided testing status verbatim>
"
	local effective_prompt

	effective_prompt="$default_prompt

$diff_summary

Authoritative testing line to copy verbatim into the commit message:
$testing_status

$breaking_hints"
	if [ -n "$ai_extra_context" ]; then
		effective_prompt="$effective_prompt

Additional context from user: $ai_extra_context"
	fi

	echo "[git-upload] Generating AI commit message via Copilot…" >&2
	echo "[git-upload] Pre-computed risk: $computed_risk ($risk_reason)" >&2

	# Call Copilot CLI (or overridden AI command) with the constructed prompt.
	# Capture full output so we can extract the commit message.
	local ai_output
	if ! ai_output=$(GIT_UPLOAD_AI_PROMPT="$effective_prompt" eval "$ai_cmd" 2>/dev/null); then
		echo "[git-upload] AI command '$ai_cmd' failed. " \
			"Make sure GitHub Copilot CLI is installed and you have run 'copilot' " \
			"at least once to authenticate." >&2
		return 1
	fi

	# Prefer COMMIT_BEGIN/COMMIT_END block; fall back to legacy single-line COMMIT:.
	local commit_block
	commit_block=$(printf '%s\n' "$ai_output" | awk '
		$0 == "COMMIT_BEGIN" { inside=1; next }
		$0 == "COMMIT_END" { inside=0 }
		inside { print }
	')

	if [ -n "${commit_block// /}" ]; then
		# Trim trailing blank lines.
		commit_block=$(printf '%s\n' "$commit_block" | awk '{ lines[NR]=$0 } $0 !~ /^[[:space:]]*$/ { last=NR } END { for (i=1; i<=last; i++) print lines[i] }')
		# Trim leading blank lines.
		commit_block=$(printf '%s\n' "$commit_block" | awk 'BEGIN{found=0} { if (!found && $0 ~ /^[[:space:]]*$/) next; found=1; print }')
		if [ -n "${commit_block// /}" ]; then
			normalize_ai_commit_message "$commit_block" "$testing_status" "$breaking_hints" "$computed_risk" "$risk_reason"
			return 0
		fi
	fi

	local commit_line
	commit_line=$(printf '%s\n' "$ai_output" | grep '^COMMIT: ' | tail -n 1 | sed 's/^COMMIT: //')

	if [ -z "${commit_line// /}" ]; then
		echo "[git-upload] AI did not produce a COMMIT_BEGIN/COMMIT_END block (or legacy COMMIT: line); skipping AI" >&2
		return 1
	fi

	normalize_ai_commit_message "$commit_line" "$testing_status" "$breaking_hints" "$computed_risk" "$risk_reason"
}

main() {
	# Determine current branch and ensure we're not in detached HEAD.
	# NOTE: `git rev-parse --abbrev-ref HEAD` can yield surprising output in
	# brand-new repos (unborn branch / no commits yet). `git symbolic-ref` is
	# the reliable way to identify the branch HEAD points to.
	current_branch=$(git symbolic-ref -q --short HEAD 2>/dev/null || echo "")
	if [ -z "$current_branch" ]; then
		# HEAD is detached (or otherwise not a symbolic ref).
		head_commit=$(git rev-parse -q --verify HEAD 2>/dev/null || echo "")
		if [ -z "$head_commit" ]; then
			echo "[git-upload] HEAD is not on a branch (and there are no commits yet)." >&2
			echo "[git-upload] Fix: create/switch to a branch first (for example: 'git switch -c main')." >&2
			exit 1
		fi

		# If the detached commit is already contained by exactly one local branch,
		# we can safely switch to it without losing work.
		containing_branches=$(git branch --contains "$head_commit" --format='%(refname:short)' 2>/dev/null || echo "")
		containing_count=$(printf '%s\n' "$containing_branches" | sed '/^$/d' | wc -l | tr -d ' ')

		if [ "$containing_count" = "1" ]; then
			current_branch=$(printf '%s\n' "$containing_branches" | sed -n '1p')
			echo "[git-upload] Detached HEAD is contained by '$current_branch'; switching to that branch…" >&2
			git switch "$current_branch" >/dev/null 2>&1 || {
				echo "[git-upload] Failed to switch to '$current_branch'." >&2
				exit 1
			}
		else
			echo "[git-upload] You are in a detached HEAD state. Checkout or create a branch before running git-upload." >&2
			echo "[git-upload] Fix (create a branch at the current commit): git switch -c <branch-name>" >&2
			exit 1
		fi
	fi

	# If this branch has an upstream, first make sure we're not behind it.
	# If we are, attempt a rebase so pushes will be fast-forward and won't
	# immediately fail due to "non-fast-forward" errors.
	if upstream_ref=$(git rev-parse --abbrev-ref --symbolic-full-name "@{u}" 2>/dev/null); then
		# Make sure we have the latest refs
		git fetch >/dev/null 2>&1 || true

		ahead_behind=$(git rev-list --left-right --count "$upstream_ref"...HEAD 2>/dev/null || echo "")
		behind_count=$(print -- "$ahead_behind" | awk '{print $1}')

		if [ -n "$behind_count" ] && [ "$behind_count" -gt 0 ]; then
			echo "[git-upload] Branch '$current_branch' is $behind_count commit(s) behind its upstream. Rebasing..." >&2
			if ! git pull --rebase --autostash >/dev/null 2>&1; then
				GIT_DIR=$(git rev-parse --git-dir 2>/dev/null || echo .git)
				if [ -d "$GIT_DIR/rebase-merge" ] || [ -d "$GIT_DIR/rebase-apply" ] || [ -f "$GIT_DIR/MERGE_HEAD" ]; then
					echo "[git-upload] git pull --rebase --autostash stopped due to conflicts." >&2
					echo "[git-upload] Run 'git resolve' for a safe backup and conflict-resolution guidance, resolve conflicts, then rerun git-upload." >&2
				else
					echo "[git-upload] git pull --rebase --autostash failed for this branch (for example, due to local changes or a hook/network error)." >&2
					echo "[git-upload] Run 'git resolve' for a safe backup and guidance on cleaning up your working tree, then rerun git-upload." >&2
				fi
				exit 1
			fi
		fi

		# After ensuring we're up to date, do a dry-run push to catch
		# permission/branch-protection issues before burning AI tokens.
		if ! git push --dry-run >/dev/null 2>&1; then
			echo "[git-upload] Unable to push to the upstream for '$current_branch'." >&2
			echo "[git-upload] This branch may be protected or you may not have permission to push directly." >&2
			echo "[git-upload] Push via a pull request or use a different branch before running git-upload." >&2
			exit 1
		fi
	fi

	git add .

	commit_msg="${user_msg:-default commit message}"

	if [ "$use_ai" = true ]; then
		if ai_msg=$(generate_ai_message); then
			# Prefer AI message; fall back to user_msg if AI returns empty
			if [ -n "${ai_msg// /}" ]; then
				commit_msg="$ai_msg"
			fi
		else
			# AI failed (e.g., Copilot not authenticated or not available)
			if [ -z "$user_msg" ]; then
				# No manual fallback message was provided; abort commit
				echo "[git-upload] AI commit requested but unavailable and no fallback commit message provided. Aborting commit." >&2
				exit 1
			fi
		fi
	fi

	echo "[git-upload] Using commit message: $commit_msg" >&2

	git commit -m "$commit_msg"

	# Push to the current branch's upstream if configured; otherwise
	# create/set upstream on origin for the current branch.
	if git rev-parse --abbrev-ref --symbolic-full-name "@{u}" >/dev/null 2>&1; then
		git push
	else
		git push -u origin "$current_branch"
	fi
}

if [ "${GIT_UPLOAD_LIBRARY_ONLY-}" != "1" ]; then
	main "$@"
fi