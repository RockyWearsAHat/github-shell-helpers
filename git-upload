#!/bin/zsh

# git-upload
#
# Usage:
#   git-upload [-ai|--aiDiffCommitMsg [ai-context]] [commit message]
#
# Behaviour:
#   - If --aiDiffCommitMsg / -ai is passed, the script will generate a commit
#     message from the current diff using GitHub Copilot CLI by default.
#   - The first non-flag argument after -ai (if any) is treated as optional
#     extra context to append to the default AI prompt.
#   - The next non-flag argument (if any) is used as a fallback/manual
#     commit message if AI is unavailable or fails.
#   - Without -ai, the first non-flag argument is the commit message,
#     defaulting to "default commit message".
#
# AI integration:
#   Default behaviour (no configuration needed as long as `copilot` is
#   installed and on PATH):
#     - The script builds a prompt like:
#         "examine differences between the last known push to this repository
#          and the current state, write a clean concise one-three line message
#          for this diff"
#       and, if you provided [ai-context] after -ai, appends that text.
#     - It then calls GitHub Copilot CLI non-interactively (write/shell tools denied):
#         copilot -s --deny-tool write --deny-tool shell -p "$GIT_UPLOAD_AI_PROMPT"
#
#   Advanced: you may optionally override the full AI command via
#   GIT_UPLOAD_AI_CMD for a single shell/session, but this is not required.

set -euo pipefail

use_ai=false
user_msg=""
ai_extra_context=""

DEFAULT_AI_CMD='copilot -s --model gpt-5.1-codex --deny-tool write --deny-tool shell -p "$GIT_UPLOAD_AI_PROMPT"'

# Progress spinner for long-running operations
_spinner_pid=""
start_spinner() {
	local msg="$1"
	(
		local frames=('â ‹' 'â ™' 'â ¹' 'â ¸' 'â ¼' 'â ´' 'â ¦' 'â §' 'â ‡' 'â ')
		local i=1
		while true; do
			printf '\r\033[K[git-upload] %s %s' "${frames[$i]}" "$msg" >&2
			i=$(( (i % 10) + 1 ))
			sleep 0.1
		done
	) &
	_spinner_pid=$!
	disown $_spinner_pid 2>/dev/null || true
}

stop_spinner() {
	local final_msg="${1:-}"
	if [ -n "$_spinner_pid" ] && kill -0 "$_spinner_pid" 2>/dev/null; then
		kill "$_spinner_pid" 2>/dev/null || true
		wait "$_spinner_pid" 2>/dev/null || true
	fi
	_spinner_pid=""
	if [ -n "$final_msg" ]; then
		printf '\r\033[K[git-upload] %s\n' "$final_msg" >&2
	else
		printf '\r\033[K' >&2
	fi
}

# Cleanup spinner on exit
trap 'stop_spinner' EXIT INT TERM

expect_ai_context_next=false

for arg in "$@"; do
	case "$arg" in
		--aiDiffCommitMsg|-ai)
			use_ai=true
			expect_ai_context_next=true
			;;
		--*)
			# Ignore other flags for now
			;;
		*)
			if [ "$expect_ai_context_next" = true ]; then
				# This non-flag arg immediately after -ai/--aiDiffCommitMsg
				# is treated as optional extra AI context.
				ai_extra_context="$arg"
				expect_ai_context_next=false
			elif [ -z "$user_msg" ]; then
				user_msg="$arg"
			fi
			;;
	esac
done


detect_vscode_test_task() {
	# Best-effort parser for .vscode/tasks.json (JSON/JSONC) to find a "test" task.
	# Prints two lines on success:
	#  1) command line (shell-quoted where needed)
	#  2) cwd (may be empty)
	local repo_root="$1"
	local tasks_file="$repo_root/.vscode/tasks.json"
	[ -f "$tasks_file" ] || return 1

	# Strip JSONC-style comments (best effort): remove /* ... */ blocks (line-based)
	# and // full-line comments. This stays dependency-free.
	local stripped
	stripped=$(sed -e '/\/\*/,/\*\//d' -e '/^[[:space:]]*\/\//d' "$tasks_file")

	printf '%s\n' "$stripped" | awk -v repo_root="$repo_root" '
		function countch(s, c,    i, n) { n=0; for (i=1; i<=length(s); i++) if (substr(s,i,1)==c) n++; return n }
		function ltrim(s) { sub(/^[[:space:]]+/, "", s); return s }
		function rtrim(s) { sub(/[[:space:]]+$/, "", s); return s }
		function trim(s) { return rtrim(ltrim(s)) }
		function subst_vars(s) {
			gsub(/\$\{workspaceFolder\}/, repo_root, s)
			gsub(/\$\{workspaceRoot\}/, repo_root, s)
			return s
		}
		function extract_string(line, key,    t) {
			t=line
			if (index(t, "\"" key "\"") == 0) return ""
			sub(".*\"" key "\"[[:space:]]*:[[:space:]]*\"", "", t)
			sub("\".*", "", t)
			return t
		}
		BEGIN {
			in_tasks=0; found_tasks=0;
			in_obj=0; depth=0;
			in_args=0;
			label=""; group=""; command=""; cwd=""; args="";
			best_cmd=""; best_cwd=""; best_rank=999;
		}
		{
			line=$0
			if (!in_tasks) {
				if (line ~ /"tasks"[[:space:]]*:/) found_tasks=1
				if (found_tasks && line ~ /\[/) in_tasks=1
			}
			if (!in_tasks) next

			if (!in_obj && line ~ /\{/) {
				in_obj=1; depth=0; in_args=0
				label=""; group=""; command=""; cwd=""; args=""
			}

			if (in_obj) {
				if (label=="") { tmp=extract_string(line, "label"); if (tmp!="") label=tmp }
				if (command=="") { tmp=extract_string(line, "command"); if (tmp!="") command=tmp }
				if (group=="") { tmp=extract_string(line, "group"); if (tmp!="") group=tmp }
				if (group=="" && line ~ /"kind"[[:space:]]*:[[:space:]]*"test"/) group="test"
				if (cwd=="") { tmp=extract_string(line, "cwd"); if (tmp!="") cwd=tmp }

				# args: capture quoted strings inside args array (best-effort)
				if (!in_args && line ~ /"args"[[:space:]]*:[[:space:]]*\[/) {
					in_args=1
					# Drop everything up to the opening '[' to avoid capturing the key name.
					sub(/.*\[/, "", line)
				}
				if (in_args) {
					work=line
					while (index(work, "\"") > 0) {
						i=index(work, "\"")
						work=substr(work, i+1)
						j=index(work, "\"")
						if (j==0) break
						arg=substr(work, 1, j-1)
						arg=subst_vars(arg)
						q=arg
						gsub(/\\/, "\\\\", q)
						gsub(/\"/, "\\\"", q)
						args = args " " "\"" q "\""
						work=substr(work, j+1)
					}
					if (line ~ /\]/) in_args=0
				}

				depth += countch($0, "{") - countch($0, "}")
				if (depth <= 0 && $0 ~ /}/) {
					lbl=tolower(trim(label))
					grp=tolower(trim(group))
					cmd=subst_vars(command)
					cw=subst_vars(cwd)
					rank=999
					if (lbl == "test") rank=0
					else if (grp == "test") rank=1
					if (cmd != "" && rank < best_rank) {
						best_rank=rank
						best_cmd=trim(cmd) args
						best_cwd=trim(cw)
					}
					in_obj=0
				}
			}
		}
		END {
			if (best_cmd != "") {
				print trim(best_cmd)
				print trim(best_cwd)
				exit 0
			}
			exit 1
		}
	'
}

detect_test_cmd() {
	local repo_root="$1"
	if [ -z "${repo_root// /}" ]; then
		repo_root="."
	fi

	repo_has_python_tests() {
		local root="$1"
		find "$root" \
			-type f \
			\( -name 'test_*.py' -o -name '*_test.py' \) \
			-not -path '*/.venv/*' \
			-not -path '*/venv/*' \
			-not -path '*/.tox/*' \
			-not -path '*/.pytest_cache/*' \
			-not -path '*/.mypy_cache/*' \
			-not -path '*/.git/*' \
			-maxdepth 6 \
			-print 2>/dev/null \
			| head -n 1 \
			| grep -q '.'
	}

	repo_has_go_tests() {
		local root="$1"
		find "$root" \
			-type f \
			-name '*_test.go' \
			-not -path '*/.git/*' \
			-maxdepth 6 \
			-print 2>/dev/null \
			| head -n 1 \
			| grep -q '.'
	}

	package_json_test_script() {
		# Best-effort: detect if package.json defines a non-placeholder scripts.test.
		# Returns 0 if present, 1 if missing/placeholder.
		local pkg="$1"
		[ -f "$pkg" ] || return 1

		local script
		script=$(awk '
			function countch(s, c,    i, n) { n=0; for (i=1; i<=length(s); i++) if (substr(s,i,1)==c) n++; return n }
			function ltrim(s) { sub(/^[[:space:]]+/, "", s); return s }
			function rtrim(s) { sub(/[[:space:]]+$/, "", s); return s }
			function trim(s) { return rtrim(ltrim(s)) }
			function extract_test_value(line,    t) {
				t=line
				if (index(t, "\"test\"") == 0) return ""
				sub(".*\"test\"[[:space:]]*:[[:space:]]*\"", "", t)
				sub("\".*", "", t)
				return t
			}
			BEGIN { in_scripts=0; depth=0; found=0; val="" }
			{
				line=$0
				if (!in_scripts) {
					if (line ~ /\"scripts\"[[:space:]]*:/) {
						in_scripts=1
						# allow same-line object start
						depth += countch(line, "{") - countch(line, "}")
						# do not next; scripts/test may be on the same line in minified JSON
					}
					if (!in_scripts) next
				}

				depth += countch(line, "{") - countch(line, "}")
				tmp=extract_test_value(line)
				if (tmp != "") { found=1; val=tmp }
				if (depth <= 0) { in_scripts=0 }
			}
			END {
				if (found) print trim(val)
			}
		' "$pkg" 2>/dev/null || true)

		if [ -z "${script// /}" ]; then
			return 1
		fi
		if printf '%s' "$script" | grep -qi 'no test specified'; then
			return 1
		fi
		return 0
	}

	repo_has_java_tests() {
		# Conservative: require a src/test tree and at least one test-like file.
		local root="$1"
		find "$root" \
			-type f \
			\(
				-path '*/src/test/*' \
				-o -path '*/src/androidTest/*'
			\) \
			\(
				-name '*Test.java' -o -name '*Tests.java' -o -name '*IT.java' \
				-o -name '*Test.kt' -o -name '*Tests.kt' -o -name '*IT.kt' \
				-o -name '*Test.groovy' -o -name '*Tests.groovy' -o -name '*IT.groovy'
			\) \
			-not -path '*/build/*' \
			-not -path '*/target/*' \
			-not -path '*/.git/*' \
			-maxdepth 8 \
			-print 2>/dev/null \
			| head -n 1 \
			| grep -q '.'
	}

	repo_has_rust_tests() {
		# Rust can have inline #[test] modules without a tests/ folder.
		# Use a shallow heuristic to avoid running cargo test in repos with no code.
		local root="$1"
		if [ -d "$root/tests" ]; then
			find "$root/tests" -type f -maxdepth 3 -print 2>/dev/null | head -n 1 | grep -q '.' && return 0
		fi
		find "$root" \
			-type f \
			\( -path '*/src/*.rs' -o -path '*/src/*/*.rs' \) \
			-not -path '*/target/*' \
			-not -path '*/.git/*' \
			-maxdepth 6 \
			-print 2>/dev/null \
			| while read -r f; do
				grep -qE '#\[[[:space:]]*test\]' "$f" 2>/dev/null && { printf '%s\n' "$f"; break; }
			done \
			| head -n 1 \
			| grep -q '.'
	}

	is_dotnet_test_project() {
		local csproj="$1"
		[ -f "$csproj" ] || return 1
		grep -qE 'Microsoft\.NET\.Test\.Sdk|<IsTestProject>[[:space:]]*true[[:space:]]*</IsTestProject>' "$csproj" 2>/dev/null
	}

	# Prints two lines on success:
	#  1) command line
	#  2) cwd (may be empty)
	#
	# Priority:
	#  1) Explicit env var (caller-controlled)
	#  2) Per-repo git config
	#  3) VS Code workspace test task (.vscode/tasks.json group: test)
	#  4) Repo-local test runner / Makefile
	#  5) Simple heuristics for common stacks
	if [ -n "${GIT_UPLOAD_TEST_CMD-}" ]; then
		printf '%s\n' "$GIT_UPLOAD_TEST_CMD"
		printf '%s' ""
		return 0
	fi

	local cfg_cmd
	cfg_cmd=$(git config --get git-upload.testCmd 2>/dev/null || echo "")
	if [ -n "${cfg_cmd// /}" ]; then
		printf '%s\n' "$cfg_cmd"
		printf '%s' ""
		return 0
	fi

	local vs_task
	if vs_task=$(detect_vscode_test_task "$repo_root" 2>/dev/null); then
		# Already prints two lines: cmdline then cwd.
		printf '%s' "$vs_task"
		return 0
	fi

	if [ -x "$repo_root/scripts/test.sh" ]; then
		printf '%s\n' './scripts/test.sh'
		printf '%s' ""
		return 0
	fi

	if { [ -f "$repo_root/Makefile" ] || [ -f "$repo_root/makefile" ]; } && command -v make >/dev/null 2>&1; then
		local mk
		mk=$([ -f "$repo_root/Makefile" ] && echo "$repo_root/Makefile" || echo "$repo_root/makefile")
		if grep -qE '^test:' "$mk" 2>/dev/null; then
			printf '%s\n' 'make test'
			printf '%s' ""
			return 0
		fi
	fi

	if [ -f "$repo_root/package.json" ]; then
		# Only run JS tests if a real scripts.test exists.
		if package_json_test_script "$repo_root/package.json"; then
			if [ -f "$repo_root/pnpm-lock.yaml" ] && command -v pnpm >/dev/null 2>&1; then
				printf '%s\n' 'pnpm test --silent'
				printf '%s' ""
				return 0
			fi
			if [ -f "$repo_root/yarn.lock" ] && command -v yarn >/dev/null 2>&1; then
				printf '%s\n' 'yarn test --silent'
				printf '%s' ""
				return 0
			fi
			if command -v npm >/dev/null 2>&1; then
				printf '%s\n' 'npm test --silent'
				printf '%s' ""
				return 0
			fi
		fi
	fi

	if { [ -f "$repo_root/pyproject.toml" ] || [ -f "$repo_root/pytest.ini" ] || [ -d "$repo_root/tests" ]; } && command -v pytest >/dev/null 2>&1; then
		# Pytest exits non-zero when no tests are collected; require at least one test file.
		if repo_has_python_tests "$repo_root"; then
			printf '%s\n' 'pytest -q'
			printf '%s' ""
			return 0
		fi
	fi

	if [ -f "$repo_root/go.mod" ] && command -v go >/dev/null 2>&1; then
		# Avoid running go test in repos with zero *_test.go files.
		if repo_has_go_tests "$repo_root"; then
			printf '%s\n' 'go test ./...'
			printf '%s' ""
			return 0
		fi
	fi

	if [ -f "$repo_root/Cargo.toml" ] && command -v cargo >/dev/null 2>&1; then
		# cargo test succeeds with zero tests, but only run when tests are likely present.
		if repo_has_rust_tests "$repo_root"; then
			printf '%s\n' 'cargo test -q'
			printf '%s' ""
			return 0
		fi
	fi

	# .NET (C#)
	if command -v dotnet >/dev/null 2>&1; then
		local sln
		sln=$(find "$repo_root" -maxdepth 2 -name '*.sln' -print 2>/dev/null | head -n 1 || true)
		if [ -n "${sln// /}" ]; then
			# Only run dotnet test when a test project exists (otherwise dotnet returns failure).
			local any_test_proj
			any_test_proj=$(find "$repo_root" -maxdepth 3 -name '*.csproj' -print 2>/dev/null | while read -r p; do
				if is_dotnet_test_project "$p"; then
					printf '%s\n' "$p"
					break
				fi
			done)
			if [ -n "${any_test_proj// /}" ]; then
				printf '%s\n' "dotnet test \"$sln\" --nologo"
				printf '%s' ""
				return 0
			fi
		fi

		local csproj
		csproj=$(find "$repo_root" -maxdepth 3 -name '*.csproj' -print 2>/dev/null | head -n 1 || true)
		if [ -n "${csproj// /}" ] && is_dotnet_test_project "$csproj"; then
			printf '%s\n' "dotnet test \"$csproj\" --nologo"
			printf '%s' ""
			return 0
		fi
	fi

	if [ -f "$repo_root/pom.xml" ] && command -v mvn >/dev/null 2>&1; then
		if repo_has_java_tests "$repo_root"; then
			printf '%s\n' 'mvn test -q'
			printf '%s' ""
			return 0
		fi
	fi

	if [ -x "$repo_root/gradlew" ]; then
		if repo_has_java_tests "$repo_root"; then
			printf '%s\n' './gradlew test -q'
			printf '%s' ""
			return 0
		fi
	fi
	if { [ -f "$repo_root/build.gradle" ] || [ -f "$repo_root/build.gradle.kts" ]; } && command -v gradle >/dev/null 2>&1; then
		if repo_has_java_tests "$repo_root"; then
			printf '%s\n' 'gradle test -q'
			printf '%s' ""
			return 0
		fi
	fi

	return 1
}

summarize_test_output() {
	# Args:
	#  1) test_cmd
	#  2) exit_code
	#  3) path to output file
	local test_cmd="$1"
	local exit_code="$2"
	local output_file="$3"

	local test_status
	if [ "$exit_code" -eq 0 ]; then
		test_status='pass'
	else
		test_status='fail'
	fi

	# If the runner emitted a machine-parseable summary, prefer it.
	# Format:
	#   TEST_SUMMARY: pass 7/7
	#   TEST_SUMMARY: fail 2/7
	#   TEST_FAIL: <name>
	local summary_line
	summary_line=$(grep '^TEST_SUMMARY: ' "$output_file" | tail -n 1 || true)
	if [ -n "${summary_line// /}" ]; then
		local sum_status
		local sum_counts
		sum_status=$(printf '%s\n' "$summary_line" | awk '{print $2}')
		sum_counts=$(printf '%s\n' "$summary_line" | awk '{print $3}')
		if [ -n "${sum_status// /}" ] && [ -n "${sum_counts// /}" ]; then
			local header="Testing: ${sum_status} (${sum_counts})"
			if [ "$sum_status" = "fail" ]; then
				local sum_failures
				sum_failures=$(grep '^TEST_FAIL: ' "$output_file" | sed 's/^TEST_FAIL: //' | head -n 10)
				if [ -n "${sum_failures// /}" ]; then
					printf '%s\n' "$header"
					printf '%s\n' "$sum_failures" | sed 's/^/- /'
					return 0
				fi
			fi
			printf '%s' "$header"
			return 0
		fi
	fi

	local passed=""
	local failed=""
	local total=""
	local count_suffix=""
	local failures=""

	case "$test_cmd" in
		pytest*)
			passed=$(grep -Eo '[0-9]+ passed' "$output_file" | tail -n 1 | awk '{print $1}')
			failed=$(grep -Eo '[0-9]+ failed' "$output_file" | tail -n 1 | awk '{print $1}')
			if [ -n "$passed" ] || [ -n "$failed" ]; then
				local p=${passed:-0}
				local f=${failed:-0}
				total=$((p + f))
				if [ "$total" -gt 0 ]; then
					if [ "$test_status" = "pass" ]; then
						count_suffix=" ($passed/$total)"
					else
						count_suffix=" ($failed/$total)"
					fi
				fi
			fi
			if [ "$test_status" = "fail" ]; then
				failures=$(grep '^FAILED ' "$output_file" | sed 's/^FAILED //' | head -n 10)
			fi
			;;
		"npm test"*|"npm test --silent"*|"yarn test"*|"yarn test --silent"*|"pnpm test"*|"pnpm test --silent"*)
			# Jest and many JS runners print a "Tests:" summary; best-effort parse.
			# Example: "Tests:       2 failed, 3 passed, 5 total"
			local tests_line
			tests_line=$(grep -E '^Tests:' "$output_file" | tail -n 1 || true)
			if [ -n "$tests_line" ]; then
				failed=$(printf '%s\n' "$tests_line" | grep -Eo '[0-9]+ failed' | awk '{print $1}' | tail -n 1)
				passed=$(printf '%s\n' "$tests_line" | grep -Eo '[0-9]+ passed' | awk '{print $1}' | tail -n 1)
				total=$(printf '%s\n' "$tests_line" | grep -Eo '[0-9]+ total' | awk '{print $1}' | tail -n 1)
				if [ -n "$total" ]; then
					if [ "$test_status" = "pass" ] && [ -n "$passed" ]; then
						count_suffix=" ($passed/$total)"
					elif [ "$test_status" = "fail" ] && [ -n "$failed" ]; then
						count_suffix=" ($failed/$total)"
					fi
				fi
			fi
			if [ "$test_status" = "fail" ]; then
				# Jest prints failing suites as: "FAIL  path/to/test"
				failures=$(grep -E '^FAIL\s+' "$output_file" | sed -E 's/^FAIL\s+//' | head -n 10)
			fi
			;;
		dotnet\ test*)
			# Common dotnet summaries:
			#   Total tests: 12. Passed: 11. Failed: 1. Skipped: 0.
			#   Passed!  - Failed: 0, Passed: 12, Skipped: 0, Total: 12, Duration: ...
			local totals
			totals=$(grep -E 'Total tests:[[:space:]]*[0-9]+' "$output_file" | tail -n 1 || true)
			if [ -z "${totals// /}" ]; then
				totals=$(grep -E 'Failed:[[:space:]]*[0-9]+, Passed:[[:space:]]*[0-9]+, Skipped:[[:space:]]*[0-9]+, Total:[[:space:]]*[0-9]+' "$output_file" | tail -n 1 || true)
			fi
			if [ -n "${totals// /}" ]; then
				local t
				local p
				local f
				t=$(printf '%s\n' "$totals" | { grep -Eo 'Total tests:[[:space:]]*[0-9]+' || true; } | grep -Eo '[0-9]+' | tail -n 1 || true)
				if [ -z "${t// /}" ]; then
					t=$(printf '%s\n' "$totals" | { grep -Eo 'Total:[[:space:]]*[0-9]+' || true; } | grep -Eo '[0-9]+' | tail -n 1 || true)
				fi
				p=$(printf '%s\n' "$totals" | { grep -Eo 'Passed:[[:space:]]*[0-9]+' || true; } | grep -Eo '[0-9]+' | tail -n 1 || true)
				f=$(printf '%s\n' "$totals" | { grep -Eo 'Failed:[[:space:]]*[0-9]+' || true; } | grep -Eo '[0-9]+' | tail -n 1 || true)
				if [ -n "${t// /}" ]; then
					if [ "$test_status" = "pass" ] && [ -n "${p// /}" ]; then
						count_suffix=" ($p/$t)"
					elif [ "$test_status" = "fail" ] && [ -n "${f// /}" ]; then
						count_suffix=" ($f/$t)"
					fi
				fi
			fi

			if [ "$test_status" = "fail" ]; then
				# Best-effort: pair "Failed <TestName>" with the first following
				# message line (xUnit commonly prints:
				#   Failed <TestName>
				#   Error Message:
				#     <message>
				# ...). Avoid emitting a blank "Message:" bullet.
				failures=$(awk '
					function trim(s) { sub(/^[[:space:]]+/, "", s); sub(/[[:space:]]+$/, "", s); return s }
					function emit(t, m) {
						if (t == "") return
						if (m == "") { print t; return }
						print t " â€” " m
					}
					BEGIN { current=""; msg=""; want_msg=0; emitted=0; count=0 }
					/^[[:space:]]*Failed[[:space:]]+/ {
						# Emit previous failure if we never got a message.
						if (current != "" && !emitted) { emit(current, msg); count++; }
						if (count >= 10) exit
						current=$0
						sub(/^[[:space:]]*Failed[[:space:]]+/, "", current)
						current=trim(current)
						msg=""; want_msg=0; emitted=0
						next
					}
					{
						if (current == "") next
						if ($0 ~ /^[[:space:]]*(Error Message:|Message:)[[:space:]]*$/) { want_msg=1; next }
						if (want_msg) {
							if ($0 ~ /^[[:space:]]*$/) next
							msg=$0
							msg=trim(msg)
							emit(current, msg)
							emitted=1
							count++
							if (count >= 10) exit
							current=""; msg=""; want_msg=0
							next
						}
					}
					END {
						if (count < 10 && current != "" && !emitted) emit(current, msg)
					}
				' "$output_file")
			fi
			;;
		*)
			# Unknown runner; we will attempt generic parsing below.
			;;
	esac

	# Generic parsing fallback for wrappers like `make test`.
	# Only compute counts if we don't already have a suffix.
	if [ -z "${count_suffix// /}" ]; then
		# pytest-style summary often appears even when invoked via a wrapper.
		passed=$(grep -Eo '[0-9]+ passed' "$output_file" | tail -n 1 | awk '{print $1}' || true)
		failed=$(grep -Eo '[0-9]+ failed' "$output_file" | tail -n 1 | awk '{print $1}' || true)
		if [ -n "${passed// /}" ] || [ -n "${failed// /}" ]; then
			local p=${passed:-0}
			local f=${failed:-0}
			total=$((p + f))
			if [ "$total" -gt 0 ]; then
				if [ "$test_status" = "pass" ]; then
					count_suffix=" ($p/$total)"
				else
					count_suffix=" ($f/$total)"
				fi
			fi
		fi

		# Python unittest summary:
		#   Ran 48 tests in 0.123s
		#   OK
		# or
		#   FAILED (failures=1, errors=0)
		if [ -z "${count_suffix// /}" ]; then
			total=$(grep -E '^Ran[[:space:]]+[0-9]+[[:space:]]+tests?' "$output_file" | tail -n 1 | grep -Eo '[0-9]+' | tail -n 1 || true)
			if [ -n "${total// /}" ]; then
				if [ "$test_status" = "pass" ]; then
					count_suffix=" ($total/$total)"
				else
					# Best-effort failures count if present.
					local uf
					local ue
					uf=$(grep -Eo 'failures=[0-9]+' "$output_file" | tail -n 1 | grep -Eo '[0-9]+' | tail -n 1 || true)
					ue=$(grep -Eo 'errors=[0-9]+' "$output_file" | tail -n 1 | grep -Eo '[0-9]+' | tail -n 1 || true)
					uf=${uf:-0}
					ue=${ue:-0}
					failed=$((uf + ue))
					count_suffix=" ($failed/$total)"
				fi
			fi
		fi

		# Rust cargo test summary:
		#   test result: ok. 48 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out
		#   test result: FAILED. 47 passed; 1 failed; ...
		if [ -z "${count_suffix// /}" ]; then
			local cargo_line
			cargo_line=$(grep -E 'test result:[[:space:]]*(ok|FAILED)\.' "$output_file" | tail -n 1 || true)
			if [ -n "${cargo_line// /}" ]; then
				local cp
				local cf
				cp=$(printf '%s\n' "$cargo_line" | grep -Eo '[0-9]+ passed' | tail -n 1 | awk '{print $1}' || true)
				cf=$(printf '%s\n' "$cargo_line" | grep -Eo '[0-9]+ failed' | tail -n 1 | awk '{print $1}' || true)
				cp=${cp:-0}
				cf=${cf:-0}
				total=$((cp + cf))
				if [ "$total" -gt 0 ]; then
					if [ "$test_status" = "pass" ]; then
						count_suffix=" ($cp/$total)"
					else
						count_suffix=" ($cf/$total)"
					fi
				fi
			fi
		fi

		# Maven Surefire:
		#   Tests run: 48, Failures: 1, Errors: 0, Skipped: 0
		if [ -z "${count_suffix// /}" ]; then
			local mvn
			mvn=$(grep -E 'Tests run:[[:space:]]*[0-9]+,[[:space:]]*Failures:[[:space:]]*[0-9]+,[[:space:]]*Errors:[[:space:]]*[0-9]+,[[:space:]]*Skipped:[[:space:]]*[0-9]+' "$output_file" | tail -n 1 || true)
			if [ -n "${mvn// /}" ]; then
				local tr
				local tf
				local te
				tr=$(printf '%s\n' "$mvn" | grep -Eo 'Tests run:[[:space:]]*[0-9]+' | grep -Eo '[0-9]+' | tail -n 1 || true)
				tf=$(printf '%s\n' "$mvn" | grep -Eo 'Failures:[[:space:]]*[0-9]+' | grep -Eo '[0-9]+' | tail -n 1 || true)
				te=$(printf '%s\n' "$mvn" | grep -Eo 'Errors:[[:space:]]*[0-9]+' | grep -Eo '[0-9]+' | tail -n 1 || true)
				tr=${tr:-0}
				tf=${tf:-0}
				te=${te:-0}
				failed=$((tf + te))
				if [ "$tr" -gt 0 ]; then
					if [ "$test_status" = "pass" ]; then
						count_suffix=" ($tr/$tr)"
					else
						count_suffix=" ($failed/$tr)"
					fi
				fi
			fi
		fi

		# Gradle:
		#   48 tests completed, 1 failed
		if [ -z "${count_suffix// /}" ]; then
			local gradle
			gradle=$(grep -E '[0-9]+ tests completed, [0-9]+ failed' "$output_file" | tail -n 1 || true)
			if [ -n "${gradle// /}" ]; then
				total=$(printf '%s\n' "$gradle" | awk '{print $1}')
				failed=$(printf '%s\n' "$gradle" | awk '{print $4}')
				if [ -n "${total// /}" ] && [ -n "${failed// /}" ]; then
					if [ "$test_status" = "pass" ]; then
						count_suffix=" ($total/$total)"
					else
						count_suffix=" ($failed/$total)"
					fi
				fi
			fi
		fi
	fi

	local header="Testing: ${test_status}${count_suffix}"
	if [ "$test_status" = "fail" ] && [ -n "${failures// /}" ]; then
		printf '%s\n' "$header"
		printf '%s\n' "$failures" | sed 's/^/- /'
		return 0
	fi

	printf '%s' "$header"
}

normalize_ai_commit_message() {
	# Args:
	#  1) commit message (may be multi-line)
	#  2) authoritative testing line (single line)
	#  3) breaking hints (multi-line; optional)
	#  4) pre-computed risk level (low|medium|high)
	#  5) pre-computed risk reason
	local msg="$1"
	local testing_line="$2"
	local breaking_hints="$3"
	local computed_risk="${4:-}"
	local risk_reason="${5:-}"

	local tmp_bullets
	tmp_bullets=$(mktemp -t git-upload-testing-bullets.XXXXXX)
	# Extract bullet lines from the authoritative testing section.
	printf '%s\n' "$testing_line" | grep '^- ' >"$tmp_bullets" 2>/dev/null || true

	local has_likely_breaking=0
	if printf '%s\n' "$breaking_hints" | grep -qi 'LIKELY BREAKING'; then
		has_likely_breaking=1
	fi
	
	# Determine test impact for breaking change logic
	local testing_degraded=0
	if printf '%s\n' "$testing_line" | grep -q '^Testing: fail (degraded)'; then
		testing_degraded=1
	fi

	# 1) Drop any AI-provided Testing lines.
	# 2) Drop duplicated failure bullets outside Testing.
	# 3) Normalize Breaking changes yes/no.
	local normalized
	normalized=$(printf '%s\n' "$msg" \
		| grep -v '^Testing:' \
		| grep -vFf "$tmp_bullets" 2>/dev/null \
		| awk -v has_likely="$has_likely_breaking" -v testing_degraded="$testing_degraded" '
			function ltrim(s) { sub(/^[[:space:]]+/, "", s); return s }
			function rtrim(s) { sub(/[[:space:]]+$/, "", s); return s }
			function trim(s) { return rtrim(ltrim(s)) }
			BEGIN { rewrote=0; in_breaking=0 }
			{
				line=$0

				if (line == "Breaking changes:") {
					in_breaking=1
					print line
					next
				}
				# If we leave the breaking section (blank line or new section header), stop de-hedging.
				if (in_breaking==1 && (line ~ /^[[:space:]]*$/ || line ~ /^[A-Z][A-Za-z ]+:[[:space:]]*$/)) {
					in_breaking=0
				}

				if (match(line, /^Breaking changes:[[:space:]]*/)) {
					rest=line
					sub(/^Breaking changes:[[:space:]]*/, "", rest)
					rest=trim(rest)

					low=tolower(rest)
					# yes/no normalization
					split(rest, parts, /[[:space:]]+/)
					yn=tolower(parts[1])
					if (!rewrote && (yn=="yes" || yn=="no")) {
						rewrote=1
						body=rest
						# drop the first word (yes/no) without relying on non-portable regex flags
						sub(/^[^[:space:]]+[[:space:]]*/, "", body)
						body=trim(body)
						sub(/^[-â€“â€”:]+[[:space:]]*/, "", body)
						if (yn=="no") {
							print "Breaking changes: none"
							next
						}
						print "Breaking changes:"
						if (length(body) > 0) print "- " body
						else print "- (details missing; review staged diff)"
						next
					}

					# Only force breaking changes if we have STRONG evidence (API removal, test degradation)
					if (low=="none" && (has_likely==1 || testing_degraded==1)) {
						print "Breaking changes:"
						print "- Likely breaking change detected; review staged diff"
						next
					}
				}
				# De-hedge bullets only when we have strong signals (test degradation)
				if (in_breaking==1 && (has_likely==1 || testing_degraded==1) && line ~ /^- /) {
					gsub(/ may now /, " will now ", line)
					gsub(/ might now /, " will now ", line)
					gsub(/ may no longer /, " will no longer ", line)
					gsub(/ might no longer /, " will no longer ", line)
					gsub(/ may be /, " is ", line)
					gsub(/ might be /, " is ", line)
					gsub(/ may /, " will ", line)
					gsub(/ might /, " will ", line)
				}
				print line
			}
		')

	rm -f "$tmp_bullets" >/dev/null 2>&1 || true

	# Append authoritative Testing section.
	if [ -n "${testing_line// /}" ]; then
		normalized="$normalized

$testing_line"
	fi

	# Determine whether the final message indicates breaking changes (bullet list).
	local breaking_kind
	breaking_kind=$(printf '%s\n' "$normalized" | awk '
		BEGIN{kind="none"}
		/^Breaking changes:[[:space:]]*none[[:space:]]*$/ { kind="none" }
		/^Breaking changes:[[:space:]]*$/ { kind="bullets" }
		END{ print kind }
	')

	# Use pre-computed risk - it's based on contextualized impact analysis
	# Only override if we detect breaking API changes with bullet list
	local final_risk="$computed_risk"
	local final_reason="$risk_reason"
	
	# Override: explicit breaking changes with likely API removal -> high
	if [ "$breaking_kind" = "bullets" ] && [ "$has_likely_breaking" -eq 1 ]; then
		final_risk="high"
		final_reason="breaking API changes detected"
	fi
	
	# Default fallback
	if [ -z "$final_risk" ]; then
		final_risk="medium"
		final_reason="review recommended"
	fi

	# Replace or append risk line with the computed risk
	if printf '%s\n' "$normalized" | grep -q '^Risk:'; then
		normalized=$(printf '%s\n' "$normalized" | awk -v risk="$final_risk" -v reason="$final_reason" '
			BEGIN{done=0}
			/^Risk:/ {
				if (!done) {
					print "Risk: " risk " (" reason ")"
					done=1
					next
				}
			}
			{ print }
		')
	else
		normalized="$normalized

Risk: $final_risk ($final_reason)"
	fi

	# Trim trailing blank lines.
	normalized=$(printf '%s\n' "$normalized" | awk '{ lines[NR]=$0 } $0 !~ /^[[:space:]]*$/ { last=NR } END { for (i=1; i<=last; i++) print lines[i] }')
	printf '%s' "$normalized"
}

# Safe parser for diff_analysis output.
# Parses key=value lines without using eval, preventing shell injection.
# Sets the following variables in the caller's scope:
#   diff_empty, files_changed, test_files_changed, config_files_changed,
#   core_files_changed, total_additions, total_deletions, code_additions,
#   code_deletions, whitespace_changes, comment_changes, syntax_error_count,
#   api_removals, signature_changes
#
# IMPORTANT: This is safe because we only assign to a whitelist of variable
# names and use printf -v to assign values as literal strings.
parse_diff_analysis() {
	local __input="$1"
	local __line __key __value
	
	# Initialize all variables to safe defaults
	diff_empty=0
	files_changed=0
	test_files_changed=0
	config_files_changed=0
	core_files_changed=0
	total_additions=0
	total_deletions=0
	code_additions=0
	code_deletions=0
	whitespace_changes=0
	comment_changes=0
	syntax_error_count=0
	api_removals=0
	signature_changes=0
	
	while IFS= read -r __line; do
		# Skip empty lines
		[ -z "$__line" ] && continue
		
		# Only process lines with = sign
		[[ "$__line" != *"="* ]] && continue
		
		# Split on first =
		__key="${__line%%=*}"
		__value="${__line#*=}"
		
		# Only allow known safe variable names (numeric values only)
		case "$__key" in
			diff_empty|files_changed|test_files_changed|config_files_changed|\
core_files_changed|total_additions|total_deletions|code_additions|\
code_deletions|whitespace_changes|comment_changes|syntax_error_count|\
api_removals|signature_changes)
				# Safely assign the value without executing it
				printf -v "$__key" '%s' "$__value"
				;;
			# Ignore syntax_errors and api_removal_details - they contain
			# arbitrary diff content that we don't need for risk scoring
			*)
				;;
		esac
	done <<< "$__input"
}

compute_diff_analysis() {
	# Comprehensive diff analysis that determines ACTUAL risk based on:
	# 1. What type of changes are being made (code vs formatting vs comments)
	# 2. The semantic impact of changes (new vs modified vs deleted functionality)
	# 3. Whether changes introduce potential issues (dead code, syntax errors)
	# 4. Test results and syntax validation
	#
	# Returns a structured analysis that can be used by AI and risk scoring.
	#
	# NOTE: Output is key=value format. Callers MUST use parse_diff_analysis()
	# instead of eval to avoid shell injection from diff content.
	
	local diff
	diff=$(git diff --cached 2>/dev/null || echo "")
	if [ -z "${diff// /}" ]; then
		printf 'diff_empty=1\n'
		return 0
	fi

	# Count actual code changes vs whitespace/formatting
	local total_additions=0
	local total_deletions=0
	local whitespace_only_changes=0
	local comment_only_changes=0
	local code_additions=0
	local code_deletions=0
	local files_changed=0
	local test_files_changed=0
	local config_files_changed=0
	local core_files_changed=0
	
	# Get list of changed files
	local changed_files
	changed_files=$(git diff --cached --name-only 2>/dev/null || echo "")
	files_changed=$(printf '%s\n' "$changed_files" | grep -c . || echo 0)
	
	# Categorize changed files
	while IFS= read -r file; do
		[ -z "$file" ] && continue
		case "$file" in
			*test*|*Test*|*spec*|*Spec*|*_test.*|*.test.*)
				test_files_changed=$((test_files_changed + 1))
				;;
			*.json|*.yaml|*.yml|*.toml|*.ini|*.cfg|*.conf|*config*|*.env*)
				config_files_changed=$((config_files_changed + 1))
				;;
			*)
				core_files_changed=$((core_files_changed + 1))
				;;
		esac
	done <<< "$changed_files"
	
	# Analyze the actual diff content
	local in_hunk=0
	local current_file=""
	while IFS= read -r line; do
		case "$line" in
			"diff --git"*)
				current_file=$(printf '%s' "$line" | sed 's/.*b\///')
				;;
			"+"[!+]*)
				total_additions=$((total_additions + 1))
				# Check if this is whitespace-only
				local content="${line#?}"
				if [ -z "${content// /}" ] || [ -z "${content//	/}" ]; then
					whitespace_only_changes=$((whitespace_only_changes + 1))
				# Check if this is a comment
				elif printf '%s' "$content" | grep -qE '^[[:space:]]*(//|#|/\*|\*|<!--)'; then
					comment_only_changes=$((comment_only_changes + 1))
				else
					code_additions=$((code_additions + 1))
				fi
				;;
			"-"[!-]*)
				total_deletions=$((total_deletions + 1))
				local content="${line#?}"
				if [ -z "${content// /}" ] || [ -z "${content//	/}" ]; then
					whitespace_only_changes=$((whitespace_only_changes + 1))
				elif printf '%s' "$content" | grep -qE '^[[:space:]]*(//|#|/\*|\*|<!--)'; then
					comment_only_changes=$((comment_only_changes + 1))
				else
					code_deletions=$((code_deletions + 1))
				fi
				;;
		esac
	done <<< "$diff"
	
	# Check for potential dead code patterns in additions
	local dead_code_signals=0
	local dead_code_details=""
	# Unreachable code after return/exit
	if printf '%s\n' "$diff" | grep -qE '^\+.*return[[:space:]]*;' && \
	   printf '%s\n' "$diff" | grep -qE '^\+[^}]*[^/].*[^/]$' 2>/dev/null; then
		# Very rough heuristic - look for code after returns that isn't closing braces
		:
	fi
	# Unused imports (added imports that might not be used)
	local added_imports
	added_imports=$(printf '%s\n' "$diff" | grep -E '^\+.*(import |from .* import |require\(|#include)' | wc -l | tr -d ' ')
	
	# Check for syntax errors in changed files (language-specific)
	# Run checks in parallel for speed, collecting results via temp files
	local syntax_errors=""
	local syntax_error_count=0
	local syntax_tmpdir
	syntax_tmpdir=$(mktemp -d -t git-upload-syntax.XXXXXX)
	local syntax_pids=()
	local tsc_checked=false
	
	while IFS= read -r file; do
		[ -z "$file" ] && continue
		[ ! -f "$file" ] && continue
		
		case "$file" in
			*.py)
				if command -v python3 >/dev/null 2>&1; then
					(
						if ! python3 -m py_compile "$file" >/dev/null 2>&1; then
							printf '%s\n' "- Python syntax error in $file" >> "$syntax_tmpdir/errors"
						fi
					) &
					syntax_pids+=($!)
				fi
				;;
			*.js|*.jsx)
				# Only check plain JavaScript files with node --check
				if command -v node >/dev/null 2>&1; then
					(
						if ! node --check "$file" >/dev/null 2>&1; then
							printf '%s\n' "- JavaScript syntax error in $file" >> "$syntax_tmpdir/errors"
						fi
					) &
					syntax_pids+=($!)
				fi
				;;
			*.ts|*.tsx)
				# TypeScript: run tsc once for entire project (not per-file)
				# Use timeout to prevent blocking on large projects
				if [ "$tsc_checked" = false ] && command -v tsc >/dev/null 2>&1; then
					tsc_checked=true
					local repo_root
					repo_root=$(git rev-parse --show-toplevel 2>/dev/null || pwd)
					if [ -f "$repo_root/tsconfig.json" ]; then
						(
							# 10-second timeout for TypeScript checking
							if command -v timeout >/dev/null 2>&1; then
								if ! timeout 10 bash -c "cd '$repo_root' && tsc --noEmit" >/dev/null 2>&1; then
									printf '%s\n' "- TypeScript syntax/compile error (see tsc output)" >> "$syntax_tmpdir/errors"
								fi
							elif command -v gtimeout >/dev/null 2>&1; then
								# macOS with coreutils
								if ! gtimeout 10 bash -c "cd '$repo_root' && tsc --noEmit" >/dev/null 2>&1; then
									printf '%s\n' "- TypeScript syntax/compile error (see tsc output)" >> "$syntax_tmpdir/errors"
								fi
							else
								# No timeout available - run without timeout but skip if it takes too long
								if ! (cd "$repo_root" && tsc --noEmit >/dev/null 2>&1); then
									printf '%s\n' "- TypeScript syntax/compile error (see tsc output)" >> "$syntax_tmpdir/errors"
								fi
							fi
						) &
						syntax_pids+=($!)
					fi
				fi
				;;
			*.sh|*.bash|*.zsh)
				if command -v bash >/dev/null 2>&1; then
					(
						if ! bash -n "$file" >/dev/null 2>&1; then
							printf '%s\n' "- Shell syntax error in $file" >> "$syntax_tmpdir/errors"
						fi
					) &
					syntax_pids+=($!)
				fi
				;;
			*.json)
				if command -v python3 >/dev/null 2>&1; then
					(
						if ! python3 -m json.tool -- "$file" >/dev/null 2>&1; then
							printf '%s\n' "- JSON syntax error in $file" >> "$syntax_tmpdir/errors"
						fi
					) &
					syntax_pids+=($!)
				fi
				;;
		esac
	done <<< "$changed_files"
	
	# Wait for all syntax checks to complete
	for pid in "${syntax_pids[@]}"; do
		wait "$pid" 2>/dev/null || true
	done
	
	# Collect syntax errors from temp file
	if [ -f "$syntax_tmpdir/errors" ]; then
		syntax_errors=$(cat "$syntax_tmpdir/errors")
		syntax_error_count=$(wc -l < "$syntax_tmpdir/errors" | tr -d ' ')
	fi
	rm -rf "$syntax_tmpdir" >/dev/null 2>&1 || true
	
	# Check for removed public APIs/exports (actual breaking changes)
	local api_removals=0
	local api_removal_details=""
	# Look for removed function/class/export declarations
	local removed_exports
	removed_exports=$(printf '%s\n' "$diff" | grep -E '^-[[:space:]]*(export |public |def |function |class |module )' | head -n 5 || true)
	if [ -n "${removed_exports// /}" ]; then
		api_removals=$(printf '%s\n' "$removed_exports" | wc -l | tr -d ' ')
		api_removal_details="$removed_exports"
	fi
	
	# Check for changed function signatures (potential breaking)
	local signature_changes=0
	# This is complex - for now, just flag if function definitions changed
	local modified_signatures
	modified_signatures=$(printf '%s\n' "$diff" | grep -E '^[-+].*(def |function |func |fn )[a-zA-Z_][a-zA-Z0-9_]*\(' | wc -l | tr -d ' ')
	if [ "$modified_signatures" -gt 0 ]; then
		signature_changes=$((modified_signatures / 2))  # Roughly - a change has both - and +
	fi
	
	# Output structured analysis
	printf 'diff_empty=0\n'
	printf 'files_changed=%d\n' "$files_changed"
	printf 'test_files_changed=%d\n' "$test_files_changed"
	printf 'config_files_changed=%d\n' "$config_files_changed"
	printf 'core_files_changed=%d\n' "$core_files_changed"
	printf 'total_additions=%d\n' "$total_additions"
	printf 'total_deletions=%d\n' "$total_deletions"
	printf 'code_additions=%d\n' "$code_additions"
	printf 'code_deletions=%d\n' "$code_deletions"
	printf 'whitespace_changes=%d\n' "$whitespace_only_changes"
	printf 'comment_changes=%d\n' "$comment_only_changes"
	printf 'syntax_error_count=%d\n' "$syntax_error_count"
	printf 'api_removals=%d\n' "$api_removals"
	printf 'signature_changes=%d\n' "$signature_changes"
	if [ -n "$syntax_errors" ]; then
		printf 'syntax_errors=%s\n' "$syntax_errors"
	fi
	if [ -n "$api_removal_details" ]; then
		printf 'api_removal_details=%s\n' "$api_removal_details"
	fi
}

compute_risk_score() {
	# Risk assessment based on CONTEXTUALIZED IMPACT of the changes:
	#
	# LOW RISK: Changes that improve the codebase or have minimal impact
	#   - Fewer test failures than before (fixing things)
	#   - All tests pass
	#   - Changes that don't introduce new problems
	#
	# MEDIUM RISK: Changes with uncertain impact that need review/testing
	#   - Same number of test failures (neutral)
	#   - Unknown baseline comparison
	#   - New functionality that hasn't been tested yet
	#
	# HIGH RISK: Changes that make things worse or introduce vulnerabilities
	#   - More test failures than before (breaking things)
	#   - Tests that were passing now fail
	#   - Syntax errors introduced
	#   - Removal of public APIs (could break consumers)
	#
	# Returns: low, medium, or high with a reason
	
	local testing_status="$1"
	local diff_analysis="$2"
	
	# Parse diff analysis
	local diff_empty=0
	local files_changed=0
	local test_files_changed=0
	local config_files_changed=0
	local core_files_changed=0
	local total_additions=0
	local total_deletions=0
	local code_additions=0
	local code_deletions=0
	local whitespace_changes=0
	local comment_changes=0
	local syntax_error_count=0
	local api_removals=0
	local signature_changes=0
	
	# Safely parse diff_analysis without using eval to prevent shell injection
	parse_diff_analysis "$diff_analysis"
	
	local risk_level="medium"  # Default to medium (unknown/uncertain)
	local risk_reasons=""
	
	# =================================================================
	# HIGH RISK: Changes that actively make things WORSE
	# =================================================================
	
	# 1. Tests degraded (more failures, or tests broke that were passing)
	#    NOTE: "fail (new tests)" is NOT degraded - it's progression (new failing tests added)
	if printf '%s\n' "$testing_status" | grep -q '^Testing: fail (degraded)'; then
		risk_level="high"
		risk_reasons="commit degraded test health"
	fi
	
	# 2. Syntax errors introduced
	if [ "$syntax_error_count" -gt 0 ]; then
		risk_level="high"
		if [ -n "$risk_reasons" ]; then
			risk_reasons="$risk_reasons, syntax errors detected"
		else
			risk_reasons="syntax errors detected"
		fi
	fi
	
	# 3. Public API/exports removed (could break downstream consumers)
	if [ "$api_removals" -gt 0 ]; then
		risk_level="high"
		if [ -n "$risk_reasons" ]; then
			risk_reasons="$risk_reasons, API exports removed"
		else
			risk_reasons="API exports removed (potential breaking change)"
		fi
	fi
	
	# =================================================================
	# LOW RISK: Changes that IMPROVE things or have minimal impact
	# =================================================================
	
	# 4. Tests improving (fewer failures than before) - this commit is FIXING things
	if printf '%s\n' "$testing_status" | grep -q '^Testing: fail (improving)'; then
		if [ "$risk_level" != "high" ]; then
			risk_level="low"
			risk_reasons="commit is fixing test failures"
		fi
	fi
	
	# 5. New tests added with failures - this is PROGRESSION, not regression
	if printf '%s\n' "$testing_status" | grep -q '^Testing: fail (new tests)'; then
		if [ "$risk_level" != "high" ]; then
			risk_level="low"
			risk_reasons="new tests added (progression, not regression)"
		fi
	fi
	
	# 6. All tests pass - codebase is healthy
	if printf '%s\n' "$testing_status" | grep -q '^Testing: pass'; then
		if [ "$risk_level" != "high" ]; then
			risk_level="low"
			risk_reasons="all tests pass"
		fi
	fi
	
	# 7. No tests configured but no syntax errors or API removals
	if printf '%s\n' "$testing_status" | grep -q '^Testing: not configured'; then
		if [ "$risk_level" != "high" ]; then
			# Without tests, we rely on other signals - default to medium for untested changes
			if [ "$risk_level" != "low" ]; then
				risk_level="medium"
				risk_reasons="changes not tested (no test suite)"
			fi
		fi
	fi
	
	# =================================================================
	# MEDIUM RISK: Uncertain impact, needs review
	# =================================================================
	
	# 8. Same number of failures (neutral impact)
	if printf '%s\n' "$testing_status" | grep -q '^Testing: fail (unchanged)'; then
		if [ "$risk_level" != "high" ] && [ "$risk_level" != "low" ]; then
			risk_level="medium"
			risk_reasons="test failures unchanged (neutral impact)"
		fi
	fi
	
	# 8. Unknown baseline (couldn't compare)
	if printf '%s\n' "$testing_status" | grep -q '^Testing: fail (unknown baseline)'; then
		if [ "$risk_level" != "high" ] && [ "$risk_level" != "low" ]; then
			risk_level="medium"
			risk_reasons="could not compare against baseline"
		fi
	fi
	
	# 9. Function signatures changed (could affect callers, but not certain)
	if [ "$risk_level" = "low" ] && [ "$signature_changes" -gt 0 ]; then
		risk_level="medium"
		risk_reasons="function signatures modified (review callers)"
	fi
	
	# Default reason if none set
	if [ -z "$risk_reasons" ]; then
		case "$risk_level" in
			low)
				risk_reasons="routine changes"
				;;
			medium)
				risk_reasons="review recommended"
				;;
			high)
				risk_reasons="significant impact detected"
				;;
		esac
	fi
	
	printf '%s|%s' "$risk_level" "$risk_reasons"
}

compute_breaking_change_hints() {
	# Smarter breaking change detection based on actual semantic impact,
	# not just keyword matching.
	
	local diff
	diff=$(git diff --cached -U3 2>/dev/null || echo "")
	if [ -z "${diff// /}" ]; then
		printf '%s' 'Breaking-change hints: no staged diff detected.'
		return 0
	fi

	local hints=""
	local has_breaking=0
	local has_potential=0
	
	# 1. Check for removed public APIs/exports (ACTUAL breaking changes)
	local removed_exports
	removed_exports=$(printf '%s\n' "$diff" | grep -E '^-[[:space:]]*(export|public|def |function |class |module\.exports)' | grep -Ev '^-\s*(//|#|/\*|\*)' | head -n 5 || true)
	if [ -n "${removed_exports// /}" ]; then
		has_breaking=1
		hints="$hints\nLikely breaking: Public API/export removed:\n$(printf '%s\n' "$removed_exports" | sed 's/^/  /')"
	fi
	
	# 2. Check for changed function/method signatures
	# Look for functions where parameters changed
	local removed_funcs
	local added_funcs
	removed_funcs=$(printf '%s\n' "$diff" | grep -E '^-[[:space:]]*(def|function|func|fn|public|private|protected)[[:space:]]+[a-zA-Z_]' | head -n 10 || true)
	added_funcs=$(printf '%s\n' "$diff" | grep -E '^\+[[:space:]]*(def|function|func|fn|public|private|protected)[[:space:]]+[a-zA-Z_]' | head -n 10 || true)
	
	if [ -n "${removed_funcs// /}" ] && [ -n "${added_funcs// /}" ]; then
		# Function definitions changed - could be signature change
		has_potential=1
		hints="$hints\nPotential breaking: Function definitions modified (check for signature changes)"
	fi
	
	# 3. Check for removed error handling (relaxed validation)
	local removed_validation
	removed_validation=$(printf '%s\n' "$diff" | grep -E '^-.*\b(throw|raise|assert|panic|error\(|Error\(|Exception)\b' | grep -Ev '^-\s*(//|#|/\*|\*)' | head -n 3 || true)
	local added_validation
	added_validation=$(printf '%s\n' "$diff" | grep -E '^\+.*\b(throw|raise|assert|panic|error\(|Error\(|Exception)\b' | grep -Ev '^\+\s*(//|#|/\*|\*)' | head -n 3 || true)
	
	if [ -n "${removed_validation// /}" ] && [ -z "${added_validation// /}" ]; then
		has_potential=1
		hints="$hints\nPotential breaking: Error handling/validation removed"
	fi
	
	# 4. Check for changed default values
	local changed_defaults
	changed_defaults=$(printf '%s\n' "$diff" | grep -E '^[-+].*=[[:space:]]*(true|false|null|nil|None|0|1|""|'"'"''"'"')' | wc -l | tr -d ' ')
	if [ "$changed_defaults" -gt 2 ]; then
		has_potential=1
		hints="$hints\nPotential: Default values modified"
	fi
	
	# 5. Do NOT flag simple keyword matches as breaking
	# The old logic flagged things like "deprecated" in comments - we don't do that anymore
	
	# Build final output
	if [ "$has_breaking" -eq 1 ]; then
		printf 'Breaking-change hints: LIKELY BREAKING changes detected:%s' "$hints"
	elif [ "$has_potential" -eq 1 ]; then
		printf 'Breaking-change hints: Potential behavior changes detected (review recommended):%s' "$hints"
	else
		printf '%s' 'Breaking-change hints: No obvious breaking-change patterns detected.'
	fi
}

compute_testing_status() {
	# Returns testing status with comparison against previous commit.
	# Format: "Testing: <status> [(<details>)]"
	# Where status can be:
	#   - "pass" - all tests pass
	#   - "fail (new)" - tests that were passing before now fail (THIS COMMIT BROKE THEM)
	#   - "fail (pre-existing)" - tests failing but they were already failing before this commit
	#   - "not configured" - no test runner detected
	
	local repo_root
	repo_root=$(git rev-parse --show-toplevel 2>/dev/null || echo "")
	if [ -z "${repo_root// /}" ]; then
		printf '%s' 'Testing: not configured'
		return 0
	fi

	local test_spec
	if ! test_spec=$(detect_test_cmd "$repo_root"); then
		printf '%s' 'Testing: not configured'
		return 0
	fi
	local test_cmd
	local test_cwd
	IFS=$'\n' read -r test_cmd test_cwd <<EOF
$test_spec
EOF
	if [ -z "${test_cmd// /}" ]; then
		printf '%s' 'Testing: not configured'
		return 0
	fi
	if [ -z "${test_cwd// /}" ]; then
		test_cwd="$repo_root"
	fi

	# Step 1: Run tests on current state (with staged changes)
	echo "" >&2
	echo "[git-upload] â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”" >&2
	echo "[git-upload] ðŸ“‹ TEST VERIFICATION" >&2
	echo "[git-upload] â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”" >&2
	echo "[git-upload] Step 1/2: Testing your changes..." >&2
	start_spinner "Running tests on current changes..."

	local tmp_current
	tmp_current=$(mktemp -t git-upload-tests-current.XXXXXX)
	local current_exit_code=0

	if (cd "$test_cwd" && eval "$test_cmd") >"$tmp_current" 2>&1; then
		current_exit_code=0
	else
		current_exit_code=$?
	fi
	
	stop_spinner

	# Check for "no tests" conditions first
	if [ "$current_exit_code" -ne 0 ]; then
		# Pytest: exit 5, plus common text (works even if invoked via wrappers).
		if grep -qiE 'collected[[:space:]]+0[[:space:]]+items|no[[:space:]]+tests[[:space:]]+ran' "$tmp_current" 2>/dev/null; then
			rm -f "$tmp_current" >/dev/null 2>&1 || true
			echo "[git-upload] Testing: not configured" >&2
			printf '%s' 'Testing: not configured'
			return 0
		fi

		# JS runners (Jest/Vitest/Mocha wrappers): "No tests found" variants.
		if grep -qiE 'no[[:space:]]+tests[[:space:]]+found|no[[:space:]]+test[[:space:]]+files[[:space:]]+found|no[[:space:]]+tests[[:space:]]+to[[:space:]]+run' "$tmp_current" 2>/dev/null; then
			rm -f "$tmp_current" >/dev/null 2>&1 || true
			echo "[git-upload] Testing: not configured" >&2
			printf '%s' 'Testing: not configured'
			return 0
		fi

		# Maven/Surefire can be configured to fail when no tests match.
		if grep -qiE 'No tests were executed|There are no tests to run|No tests to run' "$tmp_current" 2>/dev/null; then
			rm -f "$tmp_current" >/dev/null 2>&1 || true
			echo "[git-upload] Testing: not configured" >&2
			printf '%s' 'Testing: not configured'
			return 0
		fi

		# dotnet test can fail when it discovers zero tests.
		if grep -qiE 'No test is available|No tests are available|No test files were found|No test matches the given testcase filter' "$tmp_current" 2>/dev/null; then
			rm -f "$tmp_current" >/dev/null 2>&1 || true
			echo "[git-upload] Testing: not configured" >&2
			printf '%s' 'Testing: not configured'
			return 0
		fi
	fi

	# Treat "no tests collected" as not configured (common for pytest).
	if printf '%s' "$test_cmd" | grep -q '^pytest' && [ "$current_exit_code" -eq 5 ]; then
		rm -f "$tmp_current" >/dev/null 2>&1 || true
		echo "[git-upload] Testing: not configured" >&2
		printf '%s' 'Testing: not configured'
		return 0
	fi

	# Treat JS package managers' default placeholder tests as not configured.
	if printf '%s' "$test_cmd" | grep -Eq '^(npm|yarn|pnpm) test' && grep -qi 'no test specified' "$tmp_current" 2>/dev/null; then
		rm -f "$tmp_current" >/dev/null 2>&1 || true
		echo "[git-upload] Testing: not configured" >&2
		printf '%s' 'Testing: not configured'
		return 0
	fi

	# If current tests pass, we're done - no need to check baseline
	if [ "$current_exit_code" -eq 0 ]; then
		local summary
		if summary=$(summarize_test_output "$test_cmd" "$current_exit_code" "$tmp_current" 2>/dev/null); then
			:
		else
			summary='Testing: pass'
		fi
		rm -f "$tmp_current" >/dev/null 2>&1 || true
		echo "[git-upload] âœ… $summary" >&2
		echo "[git-upload] Step 2/2: Skipped (tests pass, no baseline comparison needed)" >&2
		echo "[git-upload] â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”" >&2
		echo "" >&2
		printf '%s' "$summary"
		return 0
	fi

	# Tests are failing - now we need to compare against baseline
	# to determine if this commit made things BETTER, WORSE, or SAME
	
	# Extract failure count from current test output
	local current_fail_count
	current_fail_count=$(extract_test_failure_count "$tmp_current")
	echo "[git-upload] âš ï¸  Current tests: $current_fail_count failure(s) detected" >&2
	
	# Check if we have a previous commit to compare against
	local has_head=0
	if git rev-parse HEAD >/dev/null 2>&1; then
		has_head=1
	fi
	
	local test_delta="unknown"  # worse, same, better, or unknown
	local baseline_fail_count=0
	
	if [ "$has_head" -eq 1 ]; then
		echo "[git-upload] Step 2/2: Comparing against previous commit (HEAD)..." >&2
		
		# Stash current changes (including staged), run tests on HEAD, then restore
		local stash_result
		stash_result=$(git stash push -u -m "git-upload-baseline-test" 2>&1 || echo "stash_failed")
		
		if ! printf '%s' "$stash_result" | grep -q "stash_failed"; then
			local tmp_baseline
			tmp_baseline=$(mktemp -t git-upload-tests-baseline.XXXXXX)
			
			start_spinner "Running tests on HEAD (baseline)..."
			
			local baseline_exit_code=0
			if (cd "$test_cwd" && eval "$test_cmd") >"$tmp_baseline" 2>&1; then
				baseline_exit_code=0
			else
				baseline_exit_code=$?
			fi
			
			stop_spinner
			
			# Restore the stashed changes
			echo "[git-upload] Restoring your changes..." >&2
			if ! git stash pop >/dev/null 2>&1; then
				echo "[git-upload] âš ï¸  WARNING: Failed to re-apply stashed changes after baseline tests." >&2
				echo "[git-upload]    Please inspect your working tree (e.g., 'git status') and resolve manually." >&2
			fi
			# Re-stage files after stash pop (stash pop leaves files unstaged)
			git add -A 2>/dev/null || true
			
			# Extract baseline failure count and total test count
			local baseline_total_count=0
			if [ "$baseline_exit_code" -eq 0 ]; then
				baseline_fail_count=0
				baseline_total_count=$(extract_test_total_count "$tmp_baseline")
			else
				baseline_fail_count=$(extract_test_failure_count "$tmp_baseline")
				baseline_total_count=$(extract_test_total_count "$tmp_baseline")
			fi
			
			# Also get current total for comparison
			local current_total_count
			current_total_count=$(extract_test_total_count "$tmp_current")
			
			# Compare: did this commit make things better, worse, or same?
			# Key insight: if total tests increased and that accounts for the new failures,
			# these are likely NEW TESTS (progression), not broken old tests (regression)
			local new_test_count=0
			if [ "$current_total_count" -gt "$baseline_total_count" ] 2>/dev/null; then
				new_test_count=$((current_total_count - baseline_total_count))
			fi
			
			if [ "$baseline_fail_count" -eq 0 ] && [ "$current_fail_count" -gt 0 ]; then
				# Tests WERE passing, now failing
				if [ "$new_test_count" -ge "$current_fail_count" ] 2>/dev/null && [ "$new_test_count" -gt 0 ]; then
					# All failures are from new tests = PROGRESSION (new tests added)
					test_delta="newTests"
					echo "[git-upload] ðŸŸ£ Baseline: 0 failures ($baseline_total_count tests) â†’ Current: $current_fail_count failures ($current_total_count tests)" >&2
					echo "[git-upload]    Result: NEW TESTS added (+$new_test_count), failures are from new tests (progression, not regression)" >&2
				else
					test_delta="worse"
					echo "[git-upload] ðŸ”´ Baseline: 0 failures â†’ Current: $current_fail_count failures" >&2
					echo "[git-upload]    Result: This commit BROKE previously passing tests" >&2
				fi
			elif [ "$current_fail_count" -gt "$baseline_fail_count" ]; then
				# More failures now
				local additional_failures=$((current_fail_count - baseline_fail_count))
				if [ "$new_test_count" -ge "$additional_failures" ] 2>/dev/null && [ "$new_test_count" -gt 0 ]; then
					# Additional failures explained by new tests = PROGRESSION
					test_delta="newTests"
					echo "[git-upload] ðŸŸ£ Baseline: $baseline_fail_count failures ($baseline_total_count tests) â†’ Current: $current_fail_count failures ($current_total_count tests)" >&2
					echo "[git-upload]    Result: NEW TESTS added (+$new_test_count), additional failures are from new tests" >&2
				else
					test_delta="worse"
					echo "[git-upload] ðŸ”´ Baseline: $baseline_fail_count failures â†’ Current: $current_fail_count failures" >&2
					echo "[git-upload]    Result: This commit made things WORSE" >&2
				fi
			elif [ "$current_fail_count" -lt "$baseline_fail_count" ]; then
				# Fewer failures now = BETTER (commit is fixing things!)
				test_delta="better"
				echo "[git-upload] ðŸŸ¢ Baseline: $baseline_fail_count failures â†’ Current: $current_fail_count failures" >&2
				echo "[git-upload]    Result: This commit is IMPROVING test health" >&2
			else
				# Same number of failures
				test_delta="same"
				echo "[git-upload] ðŸŸ¡ Baseline: $baseline_fail_count failures â†’ Current: $current_fail_count failures" >&2
				echo "[git-upload]    Result: No change in test health" >&2
			fi
			
			rm -f "$tmp_baseline" >/dev/null 2>&1 || true
		else
			# Couldn't stash - assume neutral to avoid false positives
			test_delta="unknown"
			echo "[git-upload] âš ï¸  Could not stash changes - baseline comparison skipped" >&2
		fi
	else
		# No previous commit - this is initial commit with failing tests
		test_delta="worse"
		echo "[git-upload] ðŸ“ Initial commit - no baseline to compare against" >&2
		echo "[git-upload] ðŸ”´ $current_fail_count failing test(s) in initial commit" >&2
	fi
	
	echo "[git-upload] â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”" >&2
	echo "" >&2

	# Build the summary with delta information
	local summary
	if summary=$(summarize_test_output "$test_cmd" "$current_exit_code" "$tmp_current" 2>/dev/null); then
		:
	else
		summary='Testing: fail'
	fi
	rm -f "$tmp_current" >/dev/null 2>&1 || true

	# Modify the summary to indicate the impact of this commit on test health
	case "$test_delta" in
		worse)
			summary=$(printf '%s' "$summary" | sed 's/^Testing: fail/Testing: fail (degraded)/')
			;;
		better)
			summary=$(printf '%s' "$summary" | sed 's/^Testing: fail/Testing: fail (improving)/')
			;;
		same)
			summary=$(printf '%s' "$summary" | sed 's/^Testing: fail/Testing: fail (unchanged)/')
			;;
		newTests)
			# New tests were added - failures are from progression, not regression
			summary=$(printf '%s' "$summary" | sed 's/^Testing: fail/Testing: fail (new tests)/')
			;;
		*)
			summary=$(printf '%s' "$summary" | sed 's/^Testing: fail/Testing: fail (unknown baseline)/')
			;;
	esac

	if [ -z "${summary// /}" ]; then
		summary='Testing: not configured'
	fi

	echo "[git-upload] $summary" >&2
	printf '%s' "$summary"
}

extract_test_failure_count() {
	# Best-effort extraction of failure count from test output
	# Returns a number (0 if can't determine)
	local output_file="$1"
	local count=0
	
	# Helper: sanitize a captured number to ensure it's a clean integer
	_sanitize_int() {
		local val="$1"
		# Strip whitespace, newlines, and non-digit characters
		val=$(printf '%s' "$val" | tr -cd '0-9' | head -c 10)
		if [ -z "$val" ]; then
			printf '0'
			return
		fi
		printf '%s' "$val"
	}
	
	# Try various patterns for different test runners
	
	# Jest/Vitest: "Tests: X failed"
	local jest_fail
	jest_fail=$(grep -oE 'Tests:[[:space:]]+[0-9]+[[:space:]]+failed' "$output_file" 2>/dev/null | grep -oE '[0-9]+' | head -n 1 || true)
	if [ -n "$jest_fail" ]; then
		_sanitize_int "$jest_fail"
		return 0
	fi
	
	# dotnet: "Failed!  - Failed:     X" or "Failed:  X" (handles multiple spaces)
	# Must check before generic pytest pattern since both have "X failed"
	local dotnet_fail
	dotnet_fail=$(grep -oE 'Failed:[[:space:]]*[0-9]+' "$output_file" 2>/dev/null | grep -oE '[0-9]+' | head -n 1 || true)
	if [ -n "$dotnet_fail" ]; then
		_sanitize_int "$dotnet_fail"
		return 0
	fi
	
	# dotnet alternate: count individual test failure lines "X [Xms]" after "Failed!"
	local dotnet_fail_count
	dotnet_fail_count=$(grep -cE '^[[:space:]]*[A-Za-z_][A-Za-z0-9_]*[[:space:]]+\[[0-9]+[[:space:]]*m?s\][[:space:]]*â€”' "$output_file" 2>/dev/null || echo "0")
	if [ "$dotnet_fail_count" -gt 0 ] 2>/dev/null; then
		_sanitize_int "$dotnet_fail_count"
		return 0
	fi
	
	# Pytest: "X failed" or "failed: X"
	local pytest_fail
	pytest_fail=$(grep -oE '[0-9]+[[:space:]]+failed' "$output_file" 2>/dev/null | grep -oE '[0-9]+' | head -n 1 || true)
	if [ -n "$pytest_fail" ]; then
		_sanitize_int "$pytest_fail"
		return 0
	fi
	
	# Go: "FAIL" lines (count them)
	local go_fail
	go_fail=$(grep -c '^--- FAIL:' "$output_file" 2>/dev/null || echo "0")
	if [ "$go_fail" -gt 0 ] 2>/dev/null; then
		_sanitize_int "$go_fail"
		return 0
	fi
	
	# Maven: "Failures: X, Errors: Y"
	local mvn_fail
	mvn_fail=$(grep -oE 'Failures:[[:space:]]*[0-9]+' "$output_file" 2>/dev/null | grep -oE '[0-9]+' | tail -n 1 || true)
	local mvn_err
	mvn_err=$(grep -oE 'Errors:[[:space:]]*[0-9]+' "$output_file" 2>/dev/null | grep -oE '[0-9]+' | tail -n 1 || true)
	if [ -n "$mvn_fail" ] || [ -n "$mvn_err" ]; then
		mvn_fail=$(_sanitize_int "${mvn_fail:-0}")
		mvn_err=$(_sanitize_int "${mvn_err:-0}")
		printf '%s' "$((mvn_fail + mvn_err))"
		return 0
	fi
	
	# Cargo/Rust: "X failed"
	local cargo_fail
	cargo_fail=$(grep -oE '[0-9]+[[:space:]]+failed' "$output_file" 2>/dev/null | grep -oE '[0-9]+' | head -n 1 || true)
	if [ -n "$cargo_fail" ]; then
		_sanitize_int "$cargo_fail"
		return 0
	fi
	
	# Fallback: count lines with common failure indicators
	local generic_fail
	generic_fail=$(grep -ciE '(FAIL|FAILED|ERROR|BROKEN)' "$output_file" 2>/dev/null || echo "0")
	generic_fail=$(_sanitize_int "$generic_fail")
	if [ "$generic_fail" -gt 0 ] 2>/dev/null; then
		printf '%s' "$generic_fail"
		return 0
	fi
	
	# Can't determine - return 1 as a fallback (at least 1 failure since tests failed)
	printf '1'
}

extract_test_total_count() {
	# Best-effort extraction of total test count from test output
	# Returns a number (0 if can't determine)
	local output_file="$1"
	
	# Helper: sanitize a captured number to ensure it's a clean integer
	_sanitize_int() {
		local val="$1"
		val=$(printf '%s' "$val" | tr -cd '0-9' | head -c 10)
		if [ -z "$val" ]; then
			printf '0'
			return
		fi
		printf '%s' "$val"
	}
	
	# Jest/Vitest: "Tests: X total"
	local jest_total
	jest_total=$(grep -oE 'Tests:[[:space:]]+.*[0-9]+[[:space:]]+total' "$output_file" 2>/dev/null | grep -oE '[0-9]+[[:space:]]+total' | grep -oE '[0-9]+' | head -n 1 || true)
	if [ -n "$jest_total" ]; then
		_sanitize_int "$jest_total"
		return 0
	fi
	
	# dotnet: "Total: X" or "Total tests: X"
	local dotnet_total
	dotnet_total=$(grep -oE 'Total:[[:space:]]*[0-9]+' "$output_file" 2>/dev/null | grep -oE '[0-9]+' | tail -n 1 || true)
	if [ -z "$dotnet_total" ]; then
		dotnet_total=$(grep -oE 'Total tests:[[:space:]]*[0-9]+' "$output_file" 2>/dev/null | grep -oE '[0-9]+' | head -n 1 || true)
	fi
	if [ -n "$dotnet_total" ]; then
		_sanitize_int "$dotnet_total"
		return 0
	fi
	
	# Pytest: "X passed" + "X failed" = total, or "collected X items"
	local pytest_collected
	pytest_collected=$(grep -oE 'collected[[:space:]]+[0-9]+[[:space:]]+items?' "$output_file" 2>/dev/null | grep -oE '[0-9]+' | head -n 1 || true)
	if [ -n "$pytest_collected" ]; then
		_sanitize_int "$pytest_collected"
		return 0
	fi
	
	# Python unittest: "Ran X tests"
	local unittest_total
	unittest_total=$(grep -oE 'Ran[[:space:]]+[0-9]+[[:space:]]+tests?' "$output_file" 2>/dev/null | grep -oE '[0-9]+' | head -n 1 || true)
	if [ -n "$unittest_total" ]; then
		_sanitize_int "$unittest_total"
		return 0
	fi
	
	# Go: count all test function results (PASS + FAIL)
	local go_total
	go_total=$(grep -cE '^--- (PASS|FAIL):' "$output_file" 2>/dev/null || echo "0")
	if [ "$go_total" -gt 0 ] 2>/dev/null; then
		_sanitize_int "$go_total"
		return 0
	fi
	
	# Maven: "Tests run: X"
	local mvn_total
	mvn_total=$(grep -oE 'Tests run:[[:space:]]*[0-9]+' "$output_file" 2>/dev/null | grep -oE '[0-9]+' | tail -n 1 || true)
	if [ -n "$mvn_total" ]; then
		_sanitize_int "$mvn_total"
		return 0
	fi
	
	# Fallback: can't determine
	printf '0'
}

generate_ai_message() {
	local ai_cmd

	if [ -n "${GIT_UPLOAD_AI_CMD-}" ]; then
		ai_cmd="$GIT_UPLOAD_AI_CMD"
	else
		ai_cmd="$DEFAULT_AI_CMD"
	fi

	local ai_binary
	ai_binary=${ai_cmd%% *}
	if ! command -v "$ai_binary" >/dev/null 2>&1; then
		echo "[git-upload] AI command '$ai_binary' not found. " \
			"Install GitHub Copilot CLI (e.g. 'brew install copilot-cli' on macOS) " \
			"and ensure it is on your PATH." >&2
		return 1
	fi

	echo "" >&2
	echo "[git-upload] â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”" >&2
	echo "[git-upload] ðŸ” ANALYZING CHANGES" >&2
	echo "[git-upload] â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”" >&2

	local testing_status
	testing_status=$(compute_testing_status)
	
	echo "[git-upload] Checking for breaking changes..." >&2
	local breaking_hints
	breaking_hints=$(compute_breaking_change_hints)
	
	# Get comprehensive diff analysis for smart risk assessment
	start_spinner "Running syntax checks & analyzing diff..."
	local diff_analysis
	diff_analysis=$(compute_diff_analysis)
	stop_spinner "âœ… Diff analysis complete"
	
	# Compute actual risk based on meaningful criteria
	local risk_result
	risk_result=$(compute_risk_score "$testing_status" "$diff_analysis")
	local computed_risk="${risk_result%%|*}"
	local risk_reason="${risk_result#*|}"
	
	# Build diff summary for AI context
	local diff_summary=""
	# Safely parse diff_analysis without using eval to prevent shell injection
	parse_diff_analysis "$diff_analysis"
	
	# Get the actual file names for context
	local changed_file_list
	changed_file_list=$(git diff --cached --name-only 2>/dev/null | head -20 || echo "")
	local file_list_display=""
	if [ -n "$changed_file_list" ]; then
		file_list_display="
Changed files:
$(printf '%s\n' "$changed_file_list" | sed 's/^/  - /')"
	fi
	
	# Get a compact stat summary
	local stat_summary
	stat_summary=$(git diff --cached --stat 2>/dev/null | tail -1 || echo "")
	
	# Get the actual diff - full content for complete context
	local actual_diff
	actual_diff=$(git diff --cached -U3 2>/dev/null || echo "")
	local diff_line_count
	diff_line_count=$(printf '%s\n' "$actual_diff" | wc -l | tr -d ' ')
	local diff_truncated=""
	# For very large diffs (>2000 lines), truncate with notice to keep prompt manageable
	if [ "$diff_line_count" -gt 2000 ]; then
		actual_diff=$(printf '%s\n' "$actual_diff" | head -2000)
		diff_truncated="(truncated - showing first 2000 of $diff_line_count lines; review full diff with 'git diff --cached')"
	fi
	
	# Load commit guidelines if they exist
	local repo_root
	repo_root=$(git rev-parse --show-toplevel 2>/dev/null || echo "")
	local guidelines=""
	if [ -f "$repo_root/.github/COMMIT_GUIDELINES.md" ]; then
		guidelines=$(cat "$repo_root/.github/COMMIT_GUIDELINES.md" 2>/dev/null | head -100 || echo "")
	fi
	
	diff_summary="Diff analysis:
- Files changed: $files_changed (core: $core_files_changed, tests: $test_files_changed, config: $config_files_changed)
- Code changes: +$code_additions/-$code_deletions lines
- Formatting/whitespace changes: $whitespace_changes lines
- Comment changes: $comment_changes lines
- Syntax errors detected: $syntax_error_count
- API/export removals: $api_removals
- Function signature changes: $signature_changes
$file_list_display

Git stat: $stat_summary

Pre-computed risk assessment: $computed_risk ($risk_reason)

ACTUAL DIFF ($diff_line_count lines)$diff_truncated:
\`\`\`diff
$actual_diff
\`\`\`"

	# Add guidelines to context if available
	if [ -n "$guidelines" ]; then
		diff_summary="PROJECT COMMIT GUIDELINES (from .github/COMMIT_GUIDELINES.md):
$guidelines

---

$diff_summary"
	fi

	local default_prompt="You are generating a git commit message for the CURRENT STAGED CHANGES.

OPERATING CONSTRAINTS:
- You are running in READ-ONLY mode with no tool access.
- You CANNOT read files, run commands, or access the filesystem.
- ALL context you need is provided in this prompt (diff, file list, analysis).
- Do NOT ask for more information or suggest running commands.
- Work ONLY with the provided diff content below.

CRITICAL: Be SPECIFIC and CONCRETE. Generic messages like 'Update defaults' or 'Refactor code' are USELESS.

Your job:
- Examine the ACTUAL DIFF provided below to understand EXACTLY what changed.
- Write a commit message that someone unfamiliar with this work can understand in 6 months.
- Name specific files, functions, features, behaviors, and values that changed.

SPECIFICITY REQUIREMENTS (MOST IMPORTANT):
- NEVER use vague phrases like: 'core code', 'core helpers', 'default values', 'one or more options', 'various improvements', 'minor changes', 'some behavior'.
- ALWAYS name the SPECIFIC thing: 'git-upload', 'the --dry-run flag', 'MAX_RETRIES from 3 to 5', 'the parseConfig() function'.
- If a new file was added, say WHAT it does: 'Add git-remerge script for re-applying failed merges' NOT 'Add new script'.
- If behavior changed, describe the ACTUAL change: 'Skip empty commits instead of erroring' NOT 'Change default behavior'.
- If you're refactoring, say WHAT was refactored and WHY: 'Extract notification logic into separate function to reduce Discord API spam' NOT 'Refactor implementation'.

Subject line rules:
- Must describe the PRIMARY concrete change, not a category.
- Good: 'Add git-remerge command with man page'
- Good: 'Fix Discord bot sending duplicate notifications on reconnect'
- Good: 'Increase upload retry limit from 3 to 5 attempts'
- BAD: 'Update defaults in core helpers' (which helpers? which defaults?)
- BAD: 'Refactor code and add comments' (what code? what's the improvement?)
- BAD: 'Change behavior of script' (which script? what behavior?)

Content requirements:
- Base everything on the staged diff (index) vs HEAD; do not invent changes.
- List EACH significant change with its SPECIFIC impact.
- For new files: describe what each file IS and DOES.
- For modified files: describe what SPECIFICALLY changed and the effect.
- For config/default changes: state the OLD value, NEW value, and WHY.
- Call out CLI/API behavior changes with concrete before/after.

Breaking changes format rules (STRICT):
	- If there are NO breaking changes, output exactly: Breaking changes: none
	- If there ARE breaking changes, output:
		Breaking changes:
		- <SPECIFIC bullet: name the flag/function/behavior and what breaks>
		(Do NOT use Yes/No. Do NOT write 'Breaking changes: yes'.)
		(BAD: 'Default values changed' - GOOD: '--verbose now defaults to true; scripts expecting quiet output will break')

RISK ASSESSMENT: Use the pre-computed risk level unless you have strong evidence to change it.
- Formatting-only, comment, and test-only changes are LOW risk.
- Use the provided testing status EXACTLY (verbatim). Do not guess.

Style requirements:
- Subject line: imperative mood; SPECIFIC; aim for <= 72 characters.
- Body: detailed with concrete specifics; prefer bullets.
- Do NOT mention checking status, diffs, tools, agents, prompts, or instructions.

Output protocol (IMPORTANT):
- Output ONLY the commit message between these exact markers, with no other text:
COMMIT_BEGIN
<subject line>

<body, multiple lines allowed>
COMMIT_END

Body structure:
Summary:
- <SPECIFIC change with file/function/feature names>
- <Another SPECIFIC change>

Why:
- <CONCRETE motivation: what problem did this solve or what improvement does it bring>

Breaking changes: <use the strict format rules above - be SPECIFIC about what breaks>

Risk: <low|medium|high> (<short rationale>)

Testing: <use the provided testing status verbatim>
"
	local effective_prompt

	effective_prompt="$default_prompt

$diff_summary

Authoritative testing line to copy verbatim into the commit message:
$testing_status

$breaking_hints"
	if [ -n "$ai_extra_context" ]; then
		effective_prompt="$effective_prompt

Additional context from user: $ai_extra_context"
	fi

	echo "" >&2
	echo "[git-upload] â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”" >&2
	echo "[git-upload] ðŸ¤– AI COMMIT MESSAGE GENERATION" >&2
	echo "[git-upload] â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”" >&2
	echo "[git-upload] Pre-computed risk: $computed_risk ($risk_reason)" >&2
	echo "" >&2

	# Call Copilot CLI with streaming output display
	local ai_output_file
	ai_output_file=$(mktemp -t git-upload-ai-output.XXXXXX)
	local ai_exit_code=0
	
	# Start the AI command in background, capturing output
	(GIT_UPLOAD_AI_PROMPT="$effective_prompt" eval "$ai_cmd" > "$ai_output_file" 2>&1) &
	local ai_pid=$!
	
	# Display streaming "thinking" output with spinner - show all thoughts
	local frames=('â ‹' 'â ™' 'â ¹' 'â ¸' 'â ¼' 'â ´' 'â ¦' 'â §' 'â ‡' 'â ')
	local frame_idx=1
	local last_line_count=0
	local printed_lines=0
	local elapsed=0
	local elapsed_sec=0
	
	while kill -0 "$ai_pid" 2>/dev/null; do
		# Read latest output and display any new non-marker lines as thoughts
		if [ -f "$ai_output_file" ]; then
			# Note: Combine local declaration with assignment to avoid zsh printing
			# variable names to stdout when inside command substitution context
			local current_line_count=$(wc -l < "$ai_output_file" 2>/dev/null | tr -d ' ' || echo "0")
			
			# Display new lines as they arrive
			if [ "$current_line_count" -gt "$printed_lines" ]; then
				# Clear the spinner line first
				printf '\r\033[K' >&2
				
				# Get and display all new non-empty, non-marker lines
				local new_lines=$(tail -n +$((printed_lines + 1)) "$ai_output_file" 2>/dev/null | \
					grep -v '^$' | \
					grep -v '^COMMIT_BEGIN$' | \
					grep -v '^COMMIT_END$' | \
					grep -v '^COMMIT: ' || echo "")
				
				if [ -n "$new_lines" ]; then
					# Print each thought line (no truncation)
					printf '%s\n' "$new_lines" | while IFS= read -r thought_line; do
						[ -n "$thought_line" ] && printf '\033[2m[git-upload] ðŸ’­ %s\033[0m\n' "$thought_line" >&2
					done
				fi
				
				printed_lines=$current_line_count
			fi
		fi
		
		# Update spinner on current line
		elapsed=$((elapsed + 1))
		elapsed_sec=$((elapsed / 7))  # roughly seconds (sleep 0.15 * 7 â‰ˆ 1s)
		printf '\r[git-upload] %s Generating commit message... (%ds)' "${frames[$frame_idx]}" "$elapsed_sec" >&2
		frame_idx=$(( (frame_idx % 10) + 1 ))
		sleep 0.15
	done
	
	# Get exit code
	wait "$ai_pid" 2>/dev/null
	ai_exit_code=$?
	
	# Clear spinner line
	printf '\r\033[K' >&2
	
	local ai_output
	ai_output=$(cat "$ai_output_file" 2>/dev/null || echo "")
	rm -f "$ai_output_file" >/dev/null 2>&1 || true
	
	if [ "$ai_exit_code" -ne 0 ] || [ -z "${ai_output// /}" ]; then
		echo "[git-upload] âŒ AI generation failed" >&2
		echo "[git-upload] AI command '$ai_cmd' failed. " \
			"Make sure GitHub Copilot CLI is installed and you have run 'copilot' " \
			"at least once to authenticate." >&2
		return 1
	fi
	
	echo "[git-upload] âœ… Commit message generated" >&2

	# Prefer COMMIT_BEGIN/COMMIT_END block; fall back to legacy single-line COMMIT:.
	local commit_block
	commit_block=$(printf '%s\n' "$ai_output" | awk '
		# Trim leading/trailing whitespace for marker detection
		{ trimmed = $0; gsub(/^[[:space:]]+|[[:space:]]+$/, "", trimmed) }
		trimmed == "COMMIT_BEGIN" { inside=1; next }
		trimmed == "COMMIT_END" { inside=0 }
		inside { print }
	')

	if [ -n "${commit_block// /}" ]; then
		# Trim trailing blank lines.
		commit_block=$(printf '%s\n' "$commit_block" | awk '{ lines[NR]=$0 } $0 !~ /^[[:space:]]*$/ { last=NR } END { for (i=1; i<=last; i++) print lines[i] }')
		# Trim leading blank lines.
		commit_block=$(printf '%s\n' "$commit_block" | awk 'BEGIN{found=0} { if (!found && $0 ~ /^[[:space:]]*$/) next; found=1; print }')
		if [ -n "${commit_block// /}" ]; then
			normalize_ai_commit_message "$commit_block" "$testing_status" "$breaking_hints" "$computed_risk" "$risk_reason"
			return 0
		fi
	fi

	local commit_line
	commit_line=$(printf '%s\n' "$ai_output" | grep -E '^[[:space:]]*COMMIT: ' | tail -n 1 | sed 's/^[[:space:]]*COMMIT: //')

	if [ -z "${commit_line// /}" ]; then
		echo "[git-upload] AI did not produce a COMMIT_BEGIN/COMMIT_END block (or legacy COMMIT: line); skipping AI" >&2
		# Debug: show first/last few lines of AI output to help diagnose parsing issues
		echo "[git-upload] DEBUG: AI output preview (first 5 lines):" >&2
		printf '%s\n' "$ai_output" | head -n 5 | sed 's/^/    /' >&2
		echo "[git-upload] DEBUG: AI output preview (last 5 lines):" >&2
		printf '%s\n' "$ai_output" | tail -n 5 | sed 's/^/    /' >&2
		return 1
	fi

	normalize_ai_commit_message "$commit_line" "$testing_status" "$breaking_hints" "$computed_risk" "$risk_reason"
}

main() {
	# Determine current branch and ensure we're not in detached HEAD.
	# NOTE: `git rev-parse --abbrev-ref HEAD` can yield surprising output in
	# brand-new repos (unborn branch / no commits yet). `git symbolic-ref` is
	# the reliable way to identify the branch HEAD points to.
	current_branch=$(git symbolic-ref -q --short HEAD 2>/dev/null || echo "")
	if [ -z "$current_branch" ]; then
		# HEAD is detached (or otherwise not a symbolic ref).
		head_commit=$(git rev-parse -q --verify HEAD 2>/dev/null || echo "")
		if [ -z "$head_commit" ]; then
			echo "[git-upload] HEAD is not on a branch (and there are no commits yet)." >&2
			echo "[git-upload] Fix: create/switch to a branch first (for example: 'git switch -c main')." >&2
			exit 1
		fi

		# If the detached commit is already contained by exactly one local branch,
		# we can safely switch to it without losing work.
		# Filter out entries that aren't real branch names (e.g., "(HEAD detached from ...)")
		containing_branches=$(git branch --contains "$head_commit" --format='%(refname:short)' 2>/dev/null \
			| grep -v '^(' \
			| sed '/^$/d' || echo "")
		containing_count=$(printf '%s\n' "$containing_branches" | sed '/^$/d' | wc -l | tr -d ' ')

		if [ "$containing_count" = "1" ]; then
			current_branch=$(printf '%s\n' "$containing_branches" | sed -n '1p')
			echo "[git-upload] Detached HEAD is contained by '$current_branch'; switching to that branchâ€¦" >&2
			git switch "$current_branch" >/dev/null 2>&1 || {
				echo "[git-upload] Failed to switch to '$current_branch'." >&2
				exit 1
			}
		elif [ "$containing_count" = "0" ]; then
			# Detached HEAD with commits not on any branch - create a branch to preserve work
			short_hash=$(git rev-parse --short HEAD 2>/dev/null || echo "unknown")
			timestamp=$(date +%Y%m%d-%H%M%S)
			new_branch="detached-work-${short_hash}-${timestamp}"
			echo "[git-upload] You are in a detached HEAD state with commits not on any branch." >&2
			echo "[git-upload] Creating branch '$new_branch' to preserve your workâ€¦" >&2
			git switch -c "$new_branch" >/dev/null 2>&1 || {
				echo "[git-upload] Failed to create branch '$new_branch'." >&2
				exit 1
			}
			current_branch="$new_branch"
			echo "[git-upload] Successfully created and switched to '$new_branch'." >&2
		else
			echo "[git-upload] You are in a detached HEAD state. The commit exists on multiple branches:" >&2
			printf '%s\n' "$containing_branches" | sed 's/^/  - /' >&2
			echo "[git-upload] Fix: checkout one of those branches (e.g., 'git switch <branch-name>') before running git-upload." >&2
			exit 1
		fi
	fi

	# If this branch has an upstream, first make sure we're not behind it.
	# If we are, attempt a rebase so pushes will be fast-forward and won't
	# immediately fail due to "non-fast-forward" errors.
	if upstream_ref=$(git rev-parse --abbrev-ref --symbolic-full-name "@{u}" 2>/dev/null); then
		# Make sure we have the latest refs
		git fetch >/dev/null 2>&1 || true

		ahead_behind=$(git rev-list --left-right --count "$upstream_ref"...HEAD 2>/dev/null || echo "")
		behind_count=$(print -- "$ahead_behind" | awk '{print $1}')

		if [ -n "$behind_count" ] && [ "$behind_count" -gt 0 ]; then
			echo "[git-upload] Branch '$current_branch' is $behind_count commit(s) behind its upstream. Rebasing..." >&2
			if ! git pull --rebase --autostash >/dev/null 2>&1; then
				GIT_DIR=$(git rev-parse --git-dir 2>/dev/null || echo .git)
				if [ -d "$GIT_DIR/rebase-merge" ] || [ -d "$GIT_DIR/rebase-apply" ] || [ -f "$GIT_DIR/MERGE_HEAD" ]; then
					echo "[git-upload] git pull --rebase --autostash stopped due to conflicts." >&2
					echo "[git-upload] Run 'git resolve' for a safe backup and conflict-resolution guidance, resolve conflicts, then rerun git-upload." >&2
				else
					echo "[git-upload] git pull --rebase --autostash failed for this branch (for example, due to local changes or a hook/network error)." >&2
					echo "[git-upload] Run 'git resolve' for a safe backup and guidance on cleaning up your working tree, then rerun git-upload." >&2
				fi
				exit 1
			fi
		fi

		# After ensuring we're up to date, do a dry-run push to catch
		# permission/branch-protection issues before burning AI tokens.
		if ! git push --dry-run >/dev/null 2>&1; then
			echo "[git-upload] Unable to push to the upstream for '$current_branch'." >&2
			echo "[git-upload] This branch may be protected or you may not have permission to push directly." >&2
			echo "[git-upload] Push via a pull request or use a different branch before running git-upload." >&2
			exit 1
		fi
	fi

	# Force stage all changes (including after stash pop which may leave files unstaged)
	git add -A

	# Check if there's anything to commit (staged changes)
	if git diff --cached --quiet 2>/dev/null; then
		# Double-check with git status in case add failed silently
		if ! git status --porcelain 2>/dev/null | grep -q '^'; then
			echo "[git-upload] Nothing to commit â€“ working tree is clean." >&2
			exit 1
		fi
		# There are changes but they're not staged - force add again
		echo "[git-upload] Re-staging unstaged changes..." >&2
		git add -A
	fi

	commit_msg="${user_msg:-default commit message}"

	if [ "$use_ai" = true ]; then
		if ai_msg=$(generate_ai_message); then
			# Prefer AI message; fall back to user_msg if AI returns empty
			if [ -n "${ai_msg// /}" ]; then
				commit_msg="$ai_msg"
			fi
		else
			# AI failed (e.g., Copilot not authenticated or not available)
			if [ -z "$user_msg" ]; then
				# No manual fallback message was provided; abort commit
				echo "[git-upload] AI commit requested but unavailable and no fallback commit message provided. Aborting commit." >&2
				exit 1
			fi
		fi
	fi

	echo "[git-upload] Using commit message: $commit_msg" >&2

	git commit -m "$commit_msg"

	# Push to the current branch's upstream if configured; otherwise
	# create/set upstream on origin for the current branch.
	if git rev-parse --abbrev-ref --symbolic-full-name "@{u}" >/dev/null 2>&1; then
		git push
	else
		git push -u origin "$current_branch"
	fi
}

if [ "${GIT_UPLOAD_LIBRARY_ONLY-}" != "1" ]; then
	main "$@"
fi